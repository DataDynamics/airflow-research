[
  {
    "fileloc_hash": 65485320130329936,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_short_circuit_operator.py",
    "last_updated": "2024-05-18 19:43:37.654427 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the ShortCircuitOperator.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import ShortCircuitOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_short_circuit_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n):\n    # [START howto_operator_short_circuit]\n    cond_true = ShortCircuitOperator(\n        task_id=\"condition_is_True\",\n        python_callable=lambda: True,\n    )\n\n    cond_false = ShortCircuitOperator(\n        task_id=\"condition_is_False\",\n        python_callable=lambda: False,\n    )\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    chain(cond_true, *ds_true)\n    chain(cond_false, *ds_false)\n    # [END howto_operator_short_circuit]\n\n    # [START howto_operator_short_circuit_trigger_rules]\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = ShortCircuitOperator(\n        task_id=\"short_circuit\", ignore_downstream_trigger_rules=False, python_callable=lambda: False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)\n    # [END howto_operator_short_circuit_trigger_rules]\n"
  },
  {
    "fileloc_hash": 43393943061151057,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_display_name.py",
    "last_updated": "2024-05-18 19:43:37.654567 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.operators.empty import EmptyOperator\n\n\n# [START dag_decorator_usage]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    dag_display_name=\"Sample DAG with Display Name\",\n)\ndef example_display_name():\n    sample_task_1 = EmptyOperator(\n        task_id=\"sample_task_1\",\n        task_display_name=\"Sample Task 1\",\n    )\n\n    @task(task_display_name=\"Sample Task 2\")\n    def sample_task_2():\n        pass\n\n    sample_task_1 >> sample_task_2()\n\n\nexample_dag = example_display_name()\n# [END dag_decorator_usage]\n"
  },
  {
    "fileloc_hash": 27444989456157044,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py",
    "last_updated": "2024-05-18 19:43:37.654679 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating the usage of ``@task.branch`` TaskFlow API decorator with depends_on_past=True,\nwhere tasks may be run or skipped on alternating runs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\n\n\n@task.branch()\ndef should_run(**kwargs) -> str:\n    \"\"\"\n    Determine which empty_task should be run based on if the execution date minute is even or odd.\n\n    :param dict kwargs: Context\n    :return: Id of the task to run\n    \"\"\"\n    print(\n        f\"------------- exec dttm = {kwargs['execution_date']} and minute = {kwargs['execution_date'].minute}\"\n    )\n    if kwargs[\"execution_date\"].minute % 2 == 0:\n        return \"empty_task_1\"\n    else:\n        return \"empty_task_2\"\n\n\nwith DAG(\n    dag_id=\"example_branch_dop_operator_v3\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    default_args={\"depends_on_past\": True},\n    tags=[\"example\"],\n) as dag:\n    cond = should_run()\n\n    empty_task_1 = EmptyOperator(task_id=\"empty_task_1\")\n    empty_task_2 = EmptyOperator(task_id=\"empty_task_2\")\n    cond >> [empty_task_1, empty_task_2]\n"
  },
  {
    "fileloc_hash": 3817399979455670,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/tutorial.py",
    "last_updated": "2024-05-18 19:43:37.654782 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\n### Tutorial Documentation\nDocumentation that goes along with the Airflow tutorial located\n[here](https://airflow.apache.org/tutorial.html)\n\"\"\"\n\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport textwrap\nfrom datetime import datetime, timedelta\n\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow.models.dag import DAG\n\n# Operators; we need this to operate!\nfrom airflow.operators.bash import BashOperator\n\n# [END import_module]\n\n\n# [START instantiate_dag]\nwith DAG(\n    \"tutorial\",\n    # [START default_args]\n    # These args will get passed on to each operator\n    # You can override them on a per-task basis during operator initialization\n    default_args={\n        \"depends_on_past\": False,\n        \"email\": [\"airflow@example.com\"],\n        \"email_on_failure\": False,\n        \"email_on_retry\": False,\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n        # 'queue': 'bash_queue',\n        # 'pool': 'backfill',\n        # 'priority_weight': 10,\n        # 'end_date': datetime(2016, 1, 1),\n        # 'wait_for_downstream': False,\n        # 'sla': timedelta(hours=2),\n        # 'execution_timeout': timedelta(seconds=300),\n        # 'on_failure_callback': some_function, # or list of functions\n        # 'on_success_callback': some_other_function, # or list of functions\n        # 'on_retry_callback': another_function, # or list of functions\n        # 'sla_miss_callback': yet_another_function, # or list of functions\n        # 'on_skipped_callback': another_function, #or list of functions\n        # 'trigger_rule': 'all_success'\n    },\n    # [END default_args]\n    description=\"A simple tutorial DAG\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [END instantiate_dag]\n\n    # t1, t2 and t3 are examples of tasks created by instantiating operators\n    # [START basic_task]\n    t1 = BashOperator(\n        task_id=\"print_date\",\n        bash_command=\"date\",\n    )\n\n    t2 = BashOperator(\n        task_id=\"sleep\",\n        depends_on_past=False,\n        bash_command=\"sleep 5\",\n        retries=3,\n    )\n    # [END basic_task]\n\n    # [START documentation]\n    t1.doc_md = textwrap.dedent(\n        \"\"\"\\\n    #### Task Documentation\n    You can document your task using the attributes `doc_md` (markdown),\n    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets\n    rendered in the UI's Task Instance Details page.\n    ![img](https://imgs.xkcd.com/comics/fixing_problems.png)\n    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)\n    \"\"\"\n    )\n\n    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR\n    dag.doc_md = \"\"\"\n    This is a documentation placed anywhere\n    \"\"\"  # otherwise, type it like this\n    # [END documentation]\n\n    # [START jinja_template]\n    templated_command = textwrap.dedent(\n        \"\"\"\n    {% for i in range(5) %}\n        echo \"{{ ds }}\"\n        echo \"{{ macros.ds_add(ds, 7)}}\"\n    {% endfor %}\n    \"\"\"\n    )\n\n    t3 = BashOperator(\n        task_id=\"templated\",\n        depends_on_past=False,\n        bash_command=templated_command,\n    )\n    # [END jinja_template]\n\n    t1 >> [t2, t3]\n# [END tutorial]\n"
  },
  {
    "fileloc_hash": 69154672499063620,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_python_decorator.py",
    "last_updated": "2024-05-18 19:43:37.654892 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating the usage of the TaskFlow API to execute Python functions natively and within a\nvirtual environment.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport sys\nimport time\nfrom pprint import pprint\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.operators.python import is_venv_installed\n\nlog = logging.getLogger(__name__)\n\nPATH_TO_PYTHON_BINARY = sys.executable\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_python_decorator():\n    # [START howto_operator_python]\n    @task(task_id=\"print_the_context\")\n    def print_context(ds=None, **kwargs):\n        \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n        pprint(kwargs)\n        print(ds)\n        return \"Whatever you return gets printed in the logs\"\n\n    run_this = print_context()\n    # [END howto_operator_python]\n\n    # [START howto_operator_python_render_sql]\n    @task(task_id=\"log_sql_query\", templates_dict={\"query\": \"sql/sample.sql\"}, templates_exts=[\".sql\"])\n    def log_sql(**kwargs):\n        log.info(\"Python task decorator query: %s\", str(kwargs[\"templates_dict\"][\"query\"]))\n\n    log_the_sql = log_sql()\n    # [END howto_operator_python_render_sql]\n\n    # [START howto_operator_python_kwargs]\n    # Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively\n    @task\n    def my_sleeping_function(random_base):\n        \"\"\"This is a function that will run within the DAG execution\"\"\"\n        time.sleep(random_base)\n\n    for i in range(5):\n        sleeping_task = my_sleeping_function.override(task_id=f\"sleep_for_{i}\")(random_base=i / 10)\n\n        run_this >> log_the_sql >> sleeping_task\n    # [END howto_operator_python_kwargs]\n\n    if not is_venv_installed():\n        log.warning(\"The virtalenv_python example task requires virtualenv, please install it.\")\n    else:\n        # [START howto_operator_python_venv]\n        @task.virtualenv(\n            task_id=\"virtualenv_python\", requirements=[\"colorama==0.4.0\"], system_site_packages=False\n        )\n        def callable_virtualenv():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            from time import sleep\n\n            from colorama import Back, Fore, Style\n\n            print(Fore.RED + \"some red text\")\n            print(Back.GREEN + \"and with a green background\")\n            print(Style.DIM + \"and in dim text\")\n            print(Style.RESET_ALL)\n            for _ in range(4):\n                print(Style.DIM + \"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        virtualenv_task = callable_virtualenv()\n        # [END howto_operator_python_venv]\n\n        sleeping_task >> virtualenv_task\n\n        # [START howto_operator_external_python]\n        @task.external_python(task_id=\"external_python\", python=PATH_TO_PYTHON_BINARY)\n        def callable_external_python():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            import sys\n            from time import sleep\n\n            print(f\"Running task via {sys.executable}\")\n            print(\"Sleeping\")\n            for _ in range(4):\n                print(\"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        external_python_task = callable_external_python()\n        # [END howto_operator_external_python]\n\n        run_this >> external_python_task >> virtualenv_task\n\n\nexample_python_decorator()\n"
  },
  {
    "fileloc_hash": 30881554713796839,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_params_ui_tutorial.py",
    "last_updated": "2024-05-18 19:43:37.655000 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"DAG demonstrating various options for a trigger form generated by DAG params.\n\nThe DAG attribute `params` is used to define a default dictionary of parameters which are usually passed\nto the DAG and which are used to render a trigger form.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport json\nfrom pathlib import Path\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param, ParamsDict\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    dag_display_name=\"Params UI tutorial\",\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example\", \"params\", \"ui\"],\n    params={\n        # Let's start simple: Standard dict values are detected from type and offered as entry form fields.\n        # Detected types are numbers, text, boolean, lists and dicts.\n        # Note that such auto-detected parameters are treated as optional (not required to contain a value)\n        \"x\": 3,\n        \"text\": \"Hello World!\",\n        \"flag\": False,\n        \"a_simple_list\": [\"one\", \"two\", \"three\", \"actually one value is made per line\"],\n        # But of course you might want to have it nicer! Let's add some description to parameters.\n        # Note if you can add any Markdown formatting to the description, you need to use the description_md\n        # attribute.\n        \"most_loved_number\": Param(\n            42,\n            type=\"integer\",\n            title=\"Your favorite number\",\n            description_md=\"Everybody should have a **favorite** number. Not only _math teachers_. \"\n            \"If you can not think of any at the moment please think of the 42 which is very famous because\"\n            \"of the book [The Hitchhiker's Guide to the Galaxy]\"\n            \"(https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#\"\n            \"The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42).\",\n        ),\n        # If you want to have a selection list box then you can use the enum feature of JSON schema\n        \"pick_one\": Param(\n            \"value 42\",\n            type=\"string\",\n            title=\"Select one Value\",\n            description=\"You can use JSON schema enum's to generate drop down selection boxes.\",\n            enum=[f\"value {i}\" for i in range(16, 64)],\n        ),\n        # You can also label the selected values via values_display attribute\n        \"pick_with_label\": Param(\n            3,\n            type=\"number\",\n            title=\"Select one Number\",\n            description=\"With drop down selections you can also have nice display labels for the values.\",\n            enum=[*range(1, 10)],\n            values_display={\n                1: \"One\",\n                2: \"Two\",\n                3: \"Three\",\n                4: \"Four - is like you take three and get one for free!\",\n                5: \"Five\",\n                6: \"Six\",\n                7: \"Seven\",\n                8: \"Eight\",\n                9: \"Nine\",\n            },\n        ),\n        # If you want to have a list box with proposals but not enforcing a fixed list\n        # then you can use the examples feature of JSON schema\n        \"proposals\": Param(\n            \"some value\",\n            type=\"string\",\n            title=\"Field with proposals\",\n            description=\"You can use JSON schema examples's to generate drop down selection boxes \"\n            \"but allow also to enter custom values. Try typing an 'a' and see options.\",\n            examples=(\n                \"Alpha,Bravo,Charlie,Delta,Echo,Foxtrot,Golf,Hotel,India,Juliett,Kilo,Lima,Mike,November,Oscar,Papa,\"\n                \"Quebec,Romeo,Sierra,Tango,Uniform,Victor,Whiskey,X-ray,Yankee,Zulu\"\n            ).split(\",\"),\n        ),\n        # If you want to select multiple items from a fixed list JSON schema des not allow to use enum\n        # In this case the type \"array\" is being used together with \"examples\" as pick list\n        \"multi_select\": Param(\n            [\"two\", \"three\"],\n            \"Select from the list of options.\",\n            type=\"array\",\n            title=\"Multi Select\",\n            examples=[\"one\", \"two\", \"three\", \"four\", \"five\"],\n        ),\n        # A multiple options selection can also be combined with values_display\n        \"multi_select_with_label\": Param(\n            [\"2\", \"3\"],\n            \"Select from the list of options. See that options can have nicer text and still technical values\"\n            \"are propagated as values during trigger to the DAG.\",\n            type=\"array\",\n            title=\"Multi Select with Labels\",\n            examples=[\"1\", \"2\", \"3\", \"4\", \"5\"],\n            values_display={\n                \"1\": \"One box of choccolate\",\n                \"2\": \"Two bananas\",\n                \"3\": \"Three apples\",\n                # Note: Value display mapping does not need to be complete.s\n            },\n        ),\n        # An array of numbers\n        \"array_of_numbers\": Param(\n            [1, 2, 3],\n            \"Only integers are accepted in this array\",\n            type=\"array\",\n            title=\"Array of numbers\",\n            items={\"type\": \"number\"},\n        ),\n        # Boolean as proper parameter with description\n        \"bool\": Param(\n            True,\n            type=\"boolean\",\n            title=\"Please confirm\",\n            description=\"A On/Off selection with a proper description.\",\n        ),\n        # Dates and Times are also supported\n        \"date_time\": Param(\n            f\"{datetime.date.today()}T{datetime.time(hour=12, minute=17, second=00)}+00:00\",\n            type=\"string\",\n            format=\"date-time\",\n            title=\"Date-Time Picker\",\n            description=\"Please select a date and time, use the button on the left for a pup-up calendar.\",\n        ),\n        \"date\": Param(\n            f\"{datetime.date.today()}\",\n            type=\"string\",\n            format=\"date\",\n            title=\"Date Picker\",\n            description=\"Please select a date, use the button on the left for a pup-up calendar. \"\n            \"See that here are no times!\",\n        ),\n        \"time\": Param(\n            f\"{datetime.time(hour=12, minute=13, second=14)}\",\n            type=[\"string\", \"null\"],\n            format=\"time\",\n            title=\"Time Picker\",\n            description=\"Please select a time, use the button on the left for a pup-up tool.\",\n        ),\n        # Fields can be required or not. If the defined fields are typed they are getting required by default\n        # (else they would not pass JSON schema validation) - to make typed fields optional you must\n        # permit the optional \"null\" type.\n        # You can omit a default value if the DAG is triggered manually\n        \"required_field\": Param(\n            # In this example we have no default value\n            # Form will enforce a value supplied by users to be able to trigger\n            type=\"string\",\n            title=\"Required text field\",\n            description=\"This field is required. You can not submit without having text in here.\",\n        ),\n        \"optional_field\": Param(\n            \"optional text, you can trigger also w/o text\",\n            type=[\"null\", \"string\"],\n            title=\"Optional text field\",\n            description_md=\"This field is optional. As field content is JSON schema validated you must \"\n            \"allow the `null` type.\",\n        ),\n        # You can arrange the entry fields in sections so that you can have a better overview for the user\n        # Therefore you can add the \"section\" attribute.\n        # The benefit of the Params class definition is that the full scope of JSON schema validation\n        # can be leveraged for form fields and they will be validated before DAG submission.\n        \"checked_text\": Param(\n            \"length-checked-field\",\n            type=\"string\",\n            title=\"Text field with length check\",\n            description_md=\"\"\"This field is required. And you need to provide something between 10 and 30\n            characters. See the JSON\n            [schema description (string)](https://json-schema.org/understanding-json-schema/reference/string.html)\n            for more details\"\"\",\n            minLength=10,\n            maxLength=20,\n            section=\"JSON Schema validation options\",\n        ),\n        \"checked_number\": Param(\n            100,\n            type=\"number\",\n            title=\"Number field with value check\",\n            description_md=\"\"\"This field is required. You need to provide any number between 64 and 128.\n            See the JSON\n            [schema description (numbers)](https://json-schema.org/understanding-json-schema/reference/numeric.html)\n            for more details\"\"\",\n            minimum=64,\n            maximum=128,\n            section=\"JSON Schema validation options\",\n        ),\n        # Some further cool stuff as advanced options are also possible\n        # You can have the user entering a dict object as a JSON with validation\n        \"object\": Param(\n            {\"key\": \"value\"},\n            type=[\"object\", \"null\"],\n            title=\"JSON entry field\",\n            section=\"Special advanced stuff with form fields\",\n        ),\n        \"array_of_objects\": Param(\n            [{\"name\": \"account_name\", \"country\": \"country_name\"}],\n            description_md=\"Array with complex objects and validation rules. \"\n            \"See [JSON Schema validation options in specs]\"\n            \"(https://json-schema.org/understanding-json-schema/reference/array.html#items).\",\n            type=\"array\",\n            title=\"JSON array field\",\n            items={\n                \"type\": \"object\",\n                \"properties\": {\"name\": {\"type\": \"string\"}, \"country_name\": {\"type\": \"string\"}},\n                \"required\": [\"name\"],\n            },\n            section=\"Special advanced stuff with form fields\",\n        ),\n        # If you want to have static parameters which are always passed and not editable by the user\n        # then you can use the JSON schema option of passing constant values. These parameters\n        # will not be displayed but passed to the DAG\n        \"hidden_secret_field\": Param(\"constant value\", const=\"constant value\"),\n    },\n) as dag:\n\n    @task(task_display_name=\"Show used parameters\")\n    def show_params(**kwargs) -> None:\n        params: ParamsDict = kwargs[\"params\"]\n        print(f\"This DAG was triggered with the following parameters:\\n\\n{json.dumps(params, indent=4)}\\n\")\n\n    show_params()\n"
  },
  {
    "fileloc_hash": 10393041317786131,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_latest_only.py",
    "last_updated": "2024-05-18 19:43:37.655086 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example of the LatestOnlyOperator\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\n\nwith DAG(\n    dag_id=\"latest_only\",\n    schedule=datetime.timedelta(hours=4),\n    start_date=datetime.datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example2\", \"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n\n    latest_only >> task1\n"
  },
  {
    "fileloc_hash": 25255556058296709,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_datasets.py",
    "last_updated": "2024-05-18 19:43:37.655179 +00:00",
    "source_code": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG for demonstrating the behavior of the Datasets feature in Airflow, including conditional and\ndataset expression-based scheduling.\n\nNotes on usage:\n\nTurn on all the DAGs.\n\ndataset_produces_1 is scheduled to run daily. Once it completes, it triggers several DAGs due to its dataset\nbeing updated. dataset_consumes_1 is triggered immediately, as it depends solely on the dataset produced by\ndataset_produces_1. consume_1_or_2_with_dataset_expressions will also be triggered, as its condition of\neither dataset_produces_1 or dataset_produces_2 being updated is satisfied with dataset_produces_1.\n\ndataset_consumes_1_and_2 will not be triggered after dataset_produces_1 runs because it requires the dataset\nfrom dataset_produces_2, which has no schedule and must be manually triggered.\n\nAfter manually triggering dataset_produces_2, several DAGs will be affected. dataset_consumes_1_and_2 should\nrun because both its dataset dependencies are now met. consume_1_and_2_with_dataset_expressions will be\ntriggered, as it requires both dataset_produces_1 and dataset_produces_2 datasets to be updated.\nconsume_1_or_2_with_dataset_expressions will be triggered again, since it's conditionally set to run when\neither dataset is updated.\n\nconsume_1_or_both_2_and_3_with_dataset_expressions demonstrates complex dataset dependency logic.\nThis DAG triggers if dataset_produces_1 is updated or if both dataset_produces_2 and dag3_dataset\nare updated. This example highlights the capability to combine updates from multiple datasets with logical\nexpressions for advanced scheduling.\n\nconditional_dataset_and_time_based_timetable illustrates the integration of time-based scheduling with\ndataset dependencies. This DAG is configured to execute either when both dataset_produces_1 and\ndataset_produces_2 datasets have been updated or according to a specific cron schedule, showcasing\nAirflow's versatility in handling mixed triggers for dataset and time-based scheduling.\n\nThe DAGs dataset_consumes_1_never_scheduled and dataset_consumes_unknown_never_scheduled will not run\nautomatically as they depend on datasets that do not get updated or are not produced by any scheduled tasks.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.datasets import Dataset\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.timetables.datasets import DatasetOrTimeSchedule\nfrom airflow.timetables.trigger import CronTriggerTimetable\n\n# [START dataset_def]\ndag1_dataset = Dataset(\"s3://dag1/output_1.txt\", extra={\"hi\": \"bye\"})\n# [END dataset_def]\ndag2_dataset = Dataset(\"s3://dag2/output_1.txt\", extra={\"hi\": \"bye\"})\ndag3_dataset = Dataset(\"s3://dag3/output_3.txt\", extra={\"hi\": \"bye\"})\n\nwith DAG(\n    dag_id=\"dataset_produces_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=\"@daily\",\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag1:\n    # [START task_outlet]\n    BashOperator(outlets=[dag1_dataset], task_id=\"producing_task_1\", bash_command=\"sleep 5\")\n    # [END task_outlet]\n\nwith DAG(\n    dag_id=\"dataset_produces_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=None,\n    tags=[\"produces\", \"dataset-scheduled\"],\n) as dag2:\n    BashOperator(outlets=[dag2_dataset], task_id=\"producing_task_2\", bash_command=\"sleep 5\")\n\n# [START dag_dep]\nwith DAG(\n    dag_id=\"dataset_consumes_1\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag3:\n    # [END dag_dep]\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_1_task/dataset_other.txt\")],\n        task_id=\"consuming_1\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_and_2\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[dag1_dataset, dag2_dataset],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag4:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_2\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_1_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        dag1_dataset,\n        Dataset(\"s3://unrelated/this-dataset-doesnt-get-triggered\"),\n    ],\n    tags=[\"consumes\", \"dataset-scheduled\"],\n) as dag5:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consuming_3\",\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"dataset_consumes_unknown_never_scheduled\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=[\n        Dataset(\"s3://unrelated/dataset3.txt\"),\n        Dataset(\"s3://unrelated/dataset_other_unknown.txt\"),\n    ],\n    tags=[\"dataset-scheduled\"],\n) as dag6:\n    BashOperator(\n        task_id=\"unrelated_task\",\n        outlets=[Dataset(\"s3://unrelated_task/dataset_other_unknown.txt\")],\n        bash_command=\"sleep 5\",\n    )\n\nwith DAG(\n    dag_id=\"consume_1_and_2_with_dataset_expressions\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=(dag1_dataset & dag2_dataset),\n) as dag5:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consume_1_and_2_with_dataset_expressions\",\n        bash_command=\"sleep 5\",\n    )\nwith DAG(\n    dag_id=\"consume_1_or_2_with_dataset_expressions\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=(dag1_dataset | dag2_dataset),\n) as dag6:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consume_1_or_2_with_dataset_expressions\",\n        bash_command=\"sleep 5\",\n    )\nwith DAG(\n    dag_id=\"consume_1_or_both_2_and_3_with_dataset_expressions\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=(dag1_dataset | (dag2_dataset & dag3_dataset)),\n) as dag7:\n    BashOperator(\n        outlets=[Dataset(\"s3://consuming_2_task/dataset_other_unknown.txt\")],\n        task_id=\"consume_1_or_both_2_and_3_with_dataset_expressions\",\n        bash_command=\"sleep 5\",\n    )\nwith DAG(\n    dag_id=\"conditional_dataset_and_time_based_timetable\",\n    catchup=False,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    schedule=DatasetOrTimeSchedule(\n        timetable=CronTriggerTimetable(\"0 1 * * 3\", timezone=\"UTC\"), datasets=(dag1_dataset & dag2_dataset)\n    ),\n    tags=[\"dataset-time-based-timetable\"],\n) as dag8:\n    BashOperator(\n        outlets=[Dataset(\"s3://dataset_time_based/dataset_other_unknown.txt\")],\n        task_id=\"conditional_dataset_and_time_based_timetable\",\n        bash_command=\"sleep 5\",\n    )\n"
  },
  {
    "fileloc_hash": 25655395153166291,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_complex.py",
    "last_updated": "2024-05-18 19:43:37.655271 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample Airflow DAG that shows the complex DAG structure.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"example_complex\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\", \"example2\", \"example3\"],\n) as dag:\n    # Create\n    create_entry_group = BashOperator(task_id=\"create_entry_group\", bash_command=\"echo create_entry_group\")\n\n    create_entry_group_result = BashOperator(\n        task_id=\"create_entry_group_result\", bash_command=\"echo create_entry_group_result\"\n    )\n\n    create_entry_group_result2 = BashOperator(\n        task_id=\"create_entry_group_result2\", bash_command=\"echo create_entry_group_result2\"\n    )\n\n    create_entry_gcs = BashOperator(task_id=\"create_entry_gcs\", bash_command=\"echo create_entry_gcs\")\n\n    create_entry_gcs_result = BashOperator(\n        task_id=\"create_entry_gcs_result\", bash_command=\"echo create_entry_gcs_result\"\n    )\n\n    create_entry_gcs_result2 = BashOperator(\n        task_id=\"create_entry_gcs_result2\", bash_command=\"echo create_entry_gcs_result2\"\n    )\n\n    create_tag = BashOperator(task_id=\"create_tag\", bash_command=\"echo create_tag\")\n\n    create_tag_result = BashOperator(task_id=\"create_tag_result\", bash_command=\"echo create_tag_result\")\n\n    create_tag_result2 = BashOperator(task_id=\"create_tag_result2\", bash_command=\"echo create_tag_result2\")\n\n    create_tag_template = BashOperator(task_id=\"create_tag_template\", bash_command=\"echo create_tag_template\")\n\n    create_tag_template_result = BashOperator(\n        task_id=\"create_tag_template_result\", bash_command=\"echo create_tag_template_result\"\n    )\n\n    create_tag_template_result2 = BashOperator(\n        task_id=\"create_tag_template_result2\", bash_command=\"echo create_tag_template_result2\"\n    )\n\n    create_tag_template_field = BashOperator(\n        task_id=\"create_tag_template_field\", bash_command=\"echo create_tag_template_field\"\n    )\n\n    create_tag_template_field_result = BashOperator(\n        task_id=\"create_tag_template_field_result\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    create_tag_template_field_result2 = BashOperator(\n        task_id=\"create_tag_template_field_result2\", bash_command=\"echo create_tag_template_field_result\"\n    )\n\n    # Delete\n    delete_entry = BashOperator(task_id=\"delete_entry\", bash_command=\"echo delete_entry\")\n    create_entry_gcs >> delete_entry\n\n    delete_entry_group = BashOperator(task_id=\"delete_entry_group\", bash_command=\"echo delete_entry_group\")\n    create_entry_group >> delete_entry_group\n\n    delete_tag = BashOperator(task_id=\"delete_tag\", bash_command=\"echo delete_tag\")\n    create_tag >> delete_tag\n\n    delete_tag_template_field = BashOperator(\n        task_id=\"delete_tag_template_field\", bash_command=\"echo delete_tag_template_field\"\n    )\n\n    delete_tag_template = BashOperator(task_id=\"delete_tag_template\", bash_command=\"echo delete_tag_template\")\n\n    # Get\n    get_entry_group = BashOperator(task_id=\"get_entry_group\", bash_command=\"echo get_entry_group\")\n\n    get_entry_group_result = BashOperator(\n        task_id=\"get_entry_group_result\", bash_command=\"echo get_entry_group_result\"\n    )\n\n    get_entry = BashOperator(task_id=\"get_entry\", bash_command=\"echo get_entry\")\n\n    get_entry_result = BashOperator(task_id=\"get_entry_result\", bash_command=\"echo get_entry_result\")\n\n    get_tag_template = BashOperator(task_id=\"get_tag_template\", bash_command=\"echo get_tag_template\")\n\n    get_tag_template_result = BashOperator(\n        task_id=\"get_tag_template_result\", bash_command=\"echo get_tag_template_result\"\n    )\n\n    # List\n    list_tags = BashOperator(task_id=\"list_tags\", bash_command=\"echo list_tags\")\n\n    list_tags_result = BashOperator(task_id=\"list_tags_result\", bash_command=\"echo list_tags_result\")\n\n    # Lookup\n    lookup_entry = BashOperator(task_id=\"lookup_entry\", bash_command=\"echo lookup_entry\")\n\n    lookup_entry_result = BashOperator(task_id=\"lookup_entry_result\", bash_command=\"echo lookup_entry_result\")\n\n    # Rename\n    rename_tag_template_field = BashOperator(\n        task_id=\"rename_tag_template_field\", bash_command=\"echo rename_tag_template_field\"\n    )\n\n    # Search\n    search_catalog = BashOperator(task_id=\"search_catalog\", bash_command=\"echo search_catalog\")\n\n    search_catalog_result = BashOperator(\n        task_id=\"search_catalog_result\", bash_command=\"echo search_catalog_result\"\n    )\n\n    # Update\n    update_entry = BashOperator(task_id=\"update_entry\", bash_command=\"echo update_entry\")\n\n    update_tag = BashOperator(task_id=\"update_tag\", bash_command=\"echo update_tag\")\n\n    update_tag_template = BashOperator(task_id=\"update_tag_template\", bash_command=\"echo update_tag_template\")\n\n    update_tag_template_field = BashOperator(\n        task_id=\"update_tag_template_field\", bash_command=\"echo update_tag_template_field\"\n    )\n\n    # Create\n    create_tasks = [\n        create_entry_group,\n        create_entry_gcs,\n        create_tag_template,\n        create_tag_template_field,\n        create_tag,\n    ]\n    chain(*create_tasks)\n\n    create_entry_group >> delete_entry_group\n    create_entry_group >> create_entry_group_result\n    create_entry_group >> create_entry_group_result2\n\n    create_entry_gcs >> delete_entry\n    create_entry_gcs >> create_entry_gcs_result\n    create_entry_gcs >> create_entry_gcs_result2\n\n    create_tag_template >> delete_tag_template_field\n    create_tag_template >> create_tag_template_result\n    create_tag_template >> create_tag_template_result2\n\n    create_tag_template_field >> delete_tag_template_field\n    create_tag_template_field >> create_tag_template_field_result\n    create_tag_template_field >> create_tag_template_field_result2\n\n    create_tag >> delete_tag\n    create_tag >> create_tag_result\n    create_tag >> create_tag_result2\n\n    # Delete\n    delete_tasks = [\n        delete_tag,\n        delete_tag_template_field,\n        delete_tag_template,\n        delete_entry_group,\n        delete_entry,\n    ]\n    chain(*delete_tasks)\n\n    # Get\n    create_tag_template >> get_tag_template >> delete_tag_template\n    get_tag_template >> get_tag_template_result\n\n    create_entry_gcs >> get_entry >> delete_entry\n    get_entry >> get_entry_result\n\n    create_entry_group >> get_entry_group >> delete_entry_group\n    get_entry_group >> get_entry_group_result\n\n    # List\n    create_tag >> list_tags >> delete_tag\n    list_tags >> list_tags_result\n\n    # Lookup\n    create_entry_gcs >> lookup_entry >> delete_entry\n    lookup_entry >> lookup_entry_result\n\n    # Rename\n    create_tag_template_field >> rename_tag_template_field >> delete_tag_template_field\n\n    # Search\n    chain(create_tasks, search_catalog, delete_tasks)\n    search_catalog >> search_catalog_result\n\n    # Update\n    create_entry_gcs >> update_entry >> delete_entry\n    create_tag >> update_tag >> delete_tag\n    create_tag_template >> update_tag_template >> delete_tag_template\n    create_tag_template_field >> update_tag_template_field >> rename_tag_template_field\n"
  },
  {
    "fileloc_hash": 24341944184899559,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/tutorial_objectstorage.py",
    "last_updated": "2024-05-18 19:43:37.655375 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport pendulum\nimport requests\n\nfrom airflow.decorators import dag, task\nfrom airflow.io.path import ObjectStoragePath\n\n# [END import_module]\n\nAPI = \"https://opendata.fmi.fi/timeseries\"\n\naq_fields = {\n    \"fmisid\": \"int32\",\n    \"time\": \"datetime64[ns]\",\n    \"AQINDEX_PT1H_avg\": \"float64\",\n    \"PM10_PT1H_avg\": \"float64\",\n    \"PM25_PT1H_avg\": \"float64\",\n    \"O3_PT1H_avg\": \"float64\",\n    \"CO_PT1H_avg\": \"float64\",\n    \"SO2_PT1H_avg\": \"float64\",\n    \"NO2_PT1H_avg\": \"float64\",\n    \"TRSC_PT1H_avg\": \"float64\",\n}\n\n# [START create_object_storage_path]\nbase = ObjectStoragePath(\"s3://aws_default@airflow-tutorial-data/\")\n# [END create_object_storage_path]\n\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef tutorial_objectstorage():\n    \"\"\"\n    ### Object Storage Tutorial Documentation\n    This is a tutorial DAG to showcase the usage of the Object Storage API.\n    Documentation that goes along with the Airflow Object Storage tutorial is\n    located\n    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/objectstorage.html)\n    \"\"\"\n\n    # [START get_air_quality_data]\n    @task\n    def get_air_quality_data(**kwargs) -> ObjectStoragePath:\n        \"\"\"\n        #### Get Air Quality Data\n        This task gets air quality data from the Finnish Meteorological Institute's\n        open data API. The data is saved as parquet.\n        \"\"\"\n        import pandas as pd\n\n        execution_date = kwargs[\"logical_date\"]\n        start_time = kwargs[\"data_interval_start\"]\n\n        params = {\n            \"format\": \"json\",\n            \"precision\": \"double\",\n            \"groupareas\": \"0\",\n            \"producer\": \"airquality_urban\",\n            \"area\": \"Uusimaa\",\n            \"param\": \",\".join(aq_fields.keys()),\n            \"starttime\": start_time.isoformat(timespec=\"seconds\"),\n            \"endtime\": execution_date.isoformat(timespec=\"seconds\"),\n            \"tz\": \"UTC\",\n        }\n\n        response = requests.get(API, params=params)\n        response.raise_for_status()\n\n        # ensure the bucket exists\n        base.mkdir(exist_ok=True)\n\n        formatted_date = execution_date.format(\"YYYYMMDD\")\n        path = base / f\"air_quality_{formatted_date}.parquet\"\n\n        df = pd.DataFrame(response.json()).astype(aq_fields)\n        with path.open(\"wb\") as file:\n            df.to_parquet(file)\n\n        return path\n\n    # [END get_air_quality_data]\n\n    # [START analyze]\n    @task\n    def analyze(path: ObjectStoragePath, **kwargs):\n        \"\"\"\n        #### Analyze\n        This task analyzes the air quality data, prints the results\n        \"\"\"\n        import duckdb\n\n        conn = duckdb.connect(database=\":memory:\")\n        conn.register_filesystem(path.fs)\n        conn.execute(f\"CREATE OR REPLACE TABLE airquality_urban AS SELECT * FROM read_parquet('{path}')\")\n\n        df2 = conn.execute(\"SELECT * FROM airquality_urban\").fetchdf()\n\n        print(df2.head())\n\n    # [END analyze]\n\n    # [START main_flow]\n    obj_path = get_air_quality_data()\n    analyze(obj_path)\n    # [END main_flow]\n\n\n# [START dag_invocation]\ntutorial_objectstorage()\n# [END dag_invocation]\n# [END tutorial]\n"
  },
  {
    "fileloc_hash": 47930866398847906,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/tutorial_taskflow_api.py",
    "last_updated": "2024-05-18 19:43:37.655467 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport json\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\n\n# [END import_module]\n\n\n# [START instantiate_dag]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef tutorial_taskflow_api():\n    \"\"\"\n    ### TaskFlow API Tutorial Documentation\n    This is a simple data pipeline example which demonstrates the use of\n    the TaskFlow API using three simple tasks for Extract, Transform, and Load.\n    Documentation that goes along with the Airflow TaskFlow API tutorial is\n    located\n    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)\n    \"\"\"\n    # [END instantiate_dag]\n\n    # [START extract]\n    @task()\n    def extract():\n        \"\"\"\n        #### Extract task\n        A simple Extract task to get data ready for the rest of the data\n        pipeline. In this case, getting data is simulated by reading from a\n        hardcoded JSON string.\n        \"\"\"\n        data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n\n        order_data_dict = json.loads(data_string)\n        return order_data_dict\n\n    # [END extract]\n\n    # [START transform]\n    @task(multiple_outputs=True)\n    def transform(order_data_dict: dict):\n        \"\"\"\n        #### Transform task\n        A simple Transform task which takes in the collection of order data and\n        computes the total order value.\n        \"\"\"\n        total_order_value = 0\n\n        for value in order_data_dict.values():\n            total_order_value += value\n\n        return {\"total_order_value\": total_order_value}\n\n    # [END transform]\n\n    # [START load]\n    @task()\n    def load(total_order_value: float):\n        \"\"\"\n        #### Load task\n        A simple Load task which takes in the result of the Transform task and\n        instead of saving it to end user review, just prints it out.\n        \"\"\"\n\n        print(f\"Total order value is: {total_order_value:.2f}\")\n\n    # [END load]\n\n    # [START main_flow]\n    order_data = extract()\n    order_summary = transform(order_data)\n    load(order_summary[\"total_order_value\"])\n    # [END main_flow]\n\n\n# [START dag_invocation]\ntutorial_taskflow_api()\n# [END dag_invocation]\n\n# [END tutorial]\n"
  },
  {
    "fileloc_hash": 25754343066752342,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py",
    "last_updated": "2024-05-18 19:43:37.655553 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of dynamic task mapping with non-TaskFlow operators.\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.models.dag import DAG\n\n\nclass AddOneOperator(BaseOperator):\n    \"\"\"A custom operator that adds one to the input.\"\"\"\n\n    def __init__(self, value, **kwargs):\n        super().__init__(**kwargs)\n        self.value = value\n\n    def execute(self, context):\n        return self.value + 1\n\n\nclass SumItOperator(BaseOperator):\n    \"\"\"A custom operator that sums the input.\"\"\"\n\n    template_fields = (\"values\",)\n\n    def __init__(self, values, **kwargs):\n        super().__init__(**kwargs)\n        self.values = values\n\n    def execute(self, context):\n        total = sum(self.values)\n        print(f\"Total was {total}\")\n        return total\n\n\nwith DAG(\n    dag_id=\"example_dynamic_task_mapping_with_no_taskflow_operators\",\n    start_date=datetime(2022, 3, 4),\n    catchup=False,\n):\n    # map the task to a list of values\n    add_one_task = AddOneOperator.partial(task_id=\"add_one\").expand(value=[1, 2, 3])\n\n    # aggregate (reduce) the mapped tasks results\n    sum_it_task = SumItOperator(task_id=\"sum_it\", values=add_one_task.output)\n"
  },
  {
    "fileloc_hash": 29470996061564375,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_params_trigger_ui.py",
    "last_updated": "2024-05-18 19:43:37.655658 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage DAG params to model a trigger UI with a user form.\n\nThis example DAG generates greetings to a list of provided names in selected languages in the logs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.models.param import Param\nfrom airflow.utils.trigger_rule import TriggerRule\n\nif TYPE_CHECKING:\n    from airflow.models.dagrun import DagRun\n    from airflow.models.taskinstance import TaskInstance\n\nwith DAG(\n    dag_id=Path(__file__).stem,\n    dag_display_name=\"Params Trigger UI\",\n    description=__doc__.partition(\".\")[0],\n    doc_md=__doc__,\n    schedule=None,\n    start_date=datetime.datetime(2022, 3, 4),\n    catchup=False,\n    tags=[\"example\", \"params\"],\n    params={\n        \"names\": Param(\n            [\"Linda\", \"Martha\", \"Thomas\"],\n            type=\"array\",\n            description=\"Define the list of names for which greetings should be generated in the logs.\"\n            \" Please have one name per line.\",\n            title=\"Names to greet\",\n        ),\n        \"english\": Param(True, type=\"boolean\", title=\"English\"),\n        \"german\": Param(True, type=\"boolean\", title=\"German (Formal)\"),\n        \"french\": Param(True, type=\"boolean\", title=\"French\"),\n    },\n) as dag:\n\n    @task(task_id=\"get_names\", task_display_name=\"Get names\")\n    def get_names(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        if \"names\" not in dag_run.conf:\n            print(\"Uuups, no names given, was no UI used to trigger?\")\n            return []\n        return dag_run.conf[\"names\"]\n\n    @task.branch(task_id=\"select_languages\", task_display_name=\"Select languages\")\n    def select_languages(**kwargs) -> list[str]:\n        ti: TaskInstance = kwargs[\"ti\"]\n        dag_run: DagRun = ti.dag_run\n        selected_languages = []\n        for lang in [\"english\", \"german\", \"french\"]:\n            if lang in dag_run.conf and dag_run.conf[lang]:\n                selected_languages.append(f\"generate_{lang}_greeting\")\n        return selected_languages\n\n    @task(task_id=\"generate_english_greeting\", task_display_name=\"Generate English greeting\")\n    def generate_english_greeting(name: str) -> str:\n        return f\"Hello {name}!\"\n\n    @task(task_id=\"generate_german_greeting\", task_display_name=\"Erzeuge Deutsche Begrung\")\n    def generate_german_greeting(name: str) -> str:\n        return f\"Sehr geehrter Herr/Frau {name}.\"\n\n    @task(task_id=\"generate_french_greeting\", task_display_name=\"Produire un message d'accueil en franais\")\n    def generate_french_greeting(name: str) -> str:\n        return f\"Bonjour {name}!\"\n\n    @task(task_id=\"print_greetings\", task_display_name=\"Print greetings\", trigger_rule=TriggerRule.ALL_DONE)\n    def print_greetings(greetings1, greetings2, greetings3) -> None:\n        for g in greetings1 or []:\n            print(g)\n        for g in greetings2 or []:\n            print(g)\n        for g in greetings3 or []:\n            print(g)\n        if not (greetings1 or greetings2 or greetings3):\n            print(\"sad, nobody to greet :-(\")\n\n    lang_select = select_languages()\n    names = get_names()\n    english_greetings = generate_english_greeting.expand(name=names)\n    german_greetings = generate_german_greeting.expand(name=names)\n    french_greetings = generate_french_greeting.expand(name=names)\n    lang_select >> [english_greetings, german_greetings, french_greetings]\n    results_print = print_greetings(english_greetings, german_greetings, french_greetings)\n"
  },
  {
    "fileloc_hash": 29871814047116245,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_external_task_marker_dag.py",
    "last_updated": "2024-05-18 19:43:37.655737 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating setting up inter-DAG dependencies using ExternalTaskSensor and\nExternalTaskMarker.\n\nIn this example, child_task1 in example_external_task_marker_child depends on parent_task in\nexample_external_task_marker_parent. When parent_task is cleared with 'Recursive' selected,\nthe presence of ExternalTaskMarker tells Airflow to clear child_task1 and its downstream tasks.\n\nExternalTaskSensor will keep poking for the status of remote ExternalTaskMarker task at a regular\ninterval till one of the following will happen:\n\nExternalTaskMarker reaches the states mentioned in the allowed_states list.\nIn this case, ExternalTaskSensor will exit with a success status code\n\nExternalTaskMarker reaches the states mentioned in the failed_states list\nIn this case, ExternalTaskSensor will raise an AirflowException and user need to handle this\nwith multiple downstream tasks\n\nExternalTaskSensor times out. In this case, ExternalTaskSensor will raise AirflowSkipException\nor AirflowSensorTimeout exception\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.external_task import ExternalTaskMarker, ExternalTaskSensor\n\nstart_date = pendulum.datetime(2021, 1, 1, tz=\"UTC\")\n\nwith DAG(\n    dag_id=\"example_external_task_marker_parent\",\n    start_date=start_date,\n    catchup=False,\n    schedule=None,\n    tags=[\"example2\"],\n) as parent_dag:\n    # [START howto_operator_external_task_marker]\n    parent_task = ExternalTaskMarker(\n        task_id=\"parent_task\",\n        external_dag_id=\"example_external_task_marker_child\",\n        external_task_id=\"child_task1\",\n    )\n    # [END howto_operator_external_task_marker]\n\nwith DAG(\n    dag_id=\"example_external_task_marker_child\",\n    start_date=start_date,\n    schedule=None,\n    catchup=False,\n    tags=[\"example2\"],\n) as child_dag:\n    # [START howto_operator_external_task_sensor]\n    child_task1 = ExternalTaskSensor(\n        task_id=\"child_task1\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_id=parent_task.task_id,\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor]\n\n    # [START howto_operator_external_task_sensor_with_task_group]\n    child_task2 = ExternalTaskSensor(\n        task_id=\"child_task2\",\n        external_dag_id=parent_dag.dag_id,\n        external_task_group_id=\"parent_dag_task_group_id\",\n        timeout=600,\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n        mode=\"reschedule\",\n    )\n    # [END howto_operator_external_task_sensor_with_task_group]\n\n    child_task3 = EmptyOperator(task_id=\"child_task3\")\n    child_task1 >> child_task2 >> child_task3\n"
  },
  {
    "fileloc_hash": 1308241633398021,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_dynamic_task_mapping.py",
    "last_updated": "2024-05-18 19:43:37.655814 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of dynamic task mapping.\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\n\nwith DAG(dag_id=\"example_dynamic_task_mapping\", start_date=datetime(2022, 3, 4)) as dag:\n\n    @task\n    def add_one(x: int):\n        return x + 1\n\n    @task\n    def sum_it(values):\n        total = sum(values)\n        print(f\"Total was {total}\")\n\n    added_values = add_one.expand(x=[1, 2, 3])\n    sum_it(added_values)\n"
  },
  {
    "fileloc_hash": 44069029579676659,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_trigger_target_dag.py",
    "last_updated": "2024-05-18 19:43:37.655912 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef run_this_func(dag_run=None):\n    \"\"\"\n    Print the payload \"message\" passed to the DagRun conf attribute.\n\n    :param dag_run: The DagRun object\n    \"\"\"\n    print(f\"Remotely received value of {dag_run.conf.get('message')} for key=message\")\n\n\nwith DAG(\n    dag_id=\"example_trigger_target_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    run_this = run_this_func()\n\n    bash_task = BashOperator(\n        task_id=\"bash_task\",\n        bash_command='echo \"Here is the message: $message\"',\n        env={\"message\": '{{ dag_run.conf.get(\"message\") }}'},\n    )\n"
  },
  {
    "fileloc_hash": 54011194403630737,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_setup_teardown.py",
    "last_updated": "2024-05-18 19:43:37.655998 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.task_group import TaskGroup\n\nwith DAG(\n    dag_id=\"example_setup_teardown\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    root_setup = BashOperator(task_id=\"root_setup\", bash_command=\"echo 'Hello from root_setup'\").as_setup()\n    root_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n    root_teardown = BashOperator(\n        task_id=\"root_teardown\", bash_command=\"echo 'Goodbye from root_teardown'\"\n    ).as_teardown(setups=root_setup)\n    root_setup >> root_normal >> root_teardown\n    with TaskGroup(\"section_1\") as section_1:\n        inner_setup = BashOperator(\n            task_id=\"taskgroup_setup\", bash_command=\"echo 'Hello from taskgroup_setup'\"\n        ).as_setup()\n        inner_normal = BashOperator(task_id=\"normal\", bash_command=\"echo 'I am just a normal task'\")\n        inner_teardown = BashOperator(\n            task_id=\"taskgroup_teardown\", bash_command=\"echo 'Hello from taskgroup_teardown'\"\n        ).as_teardown(setups=inner_setup)\n        inner_setup >> inner_normal >> inner_teardown\n    root_normal >> section_1\n"
  },
  {
    "fileloc_hash": 45392380343026429,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/tutorial_dag.py",
    "last_updated": "2024-05-18 19:43:37.656087 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\n### DAG Tutorial Documentation\nThis DAG is demonstrating an Extract -> Transform -> Load pipeline\n\"\"\"\n\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport json\nimport textwrap\n\nimport pendulum\n\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow.models.dag import DAG\n\n# Operators; we need this to operate!\nfrom airflow.operators.python import PythonOperator\n\n# [END import_module]\n\n# [START instantiate_dag]\nwith DAG(\n    \"tutorial_dag\",\n    # [START default_args]\n    # These args will get passed on to each operator\n    # You can override them on a per-task basis during operator initialization\n    default_args={\"retries\": 2},\n    # [END default_args]\n    description=\"DAG tutorial\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [END instantiate_dag]\n    # [START documentation]\n    dag.doc_md = __doc__\n    # [END documentation]\n\n    # [START extract_function]\n    def extract(**kwargs):\n        ti = kwargs[\"ti\"]\n        data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n        ti.xcom_push(\"order_data\", data_string)\n\n    # [END extract_function]\n\n    # [START transform_function]\n    def transform(**kwargs):\n        ti = kwargs[\"ti\"]\n        extract_data_string = ti.xcom_pull(task_ids=\"extract\", key=\"order_data\")\n        order_data = json.loads(extract_data_string)\n\n        total_order_value = 0\n        for value in order_data.values():\n            total_order_value += value\n\n        total_value = {\"total_order_value\": total_order_value}\n        total_value_json_string = json.dumps(total_value)\n        ti.xcom_push(\"total_order_value\", total_value_json_string)\n\n    # [END transform_function]\n\n    # [START load_function]\n    def load(**kwargs):\n        ti = kwargs[\"ti\"]\n        total_value_string = ti.xcom_pull(task_ids=\"transform\", key=\"total_order_value\")\n        total_order_value = json.loads(total_value_string)\n\n        print(total_order_value)\n\n    # [END load_function]\n\n    # [START main_flow]\n    extract_task = PythonOperator(\n        task_id=\"extract\",\n        python_callable=extract,\n    )\n    extract_task.doc_md = textwrap.dedent(\n        \"\"\"\\\n    #### Extract task\n    A simple Extract task to get data ready for the rest of the data pipeline.\n    In this case, getting data is simulated by reading from a hardcoded JSON string.\n    This data is then put into xcom, so that it can be processed by the next task.\n    \"\"\"\n    )\n\n    transform_task = PythonOperator(\n        task_id=\"transform\",\n        python_callable=transform,\n    )\n    transform_task.doc_md = textwrap.dedent(\n        \"\"\"\\\n    #### Transform task\n    A simple Transform task which takes in the collection of order data from xcom\n    and computes the total order value.\n    This computed value is then put into xcom, so that it can be processed by the next task.\n    \"\"\"\n    )\n\n    load_task = PythonOperator(\n        task_id=\"load\",\n        python_callable=load,\n    )\n    load_task.doc_md = textwrap.dedent(\n        \"\"\"\\\n    #### Load task\n    A simple Load task which takes in the result of the Transform task, by reading it\n    from xcom and instead of saving it to end user review, just prints it out.\n    \"\"\"\n    )\n\n    extract_task >> transform_task >> load_task\n\n# [END main_flow]\n\n# [END tutorial]\n"
  },
  {
    "fileloc_hash": 38605115227478470,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_skip_dag.py",
    "last_updated": "2024-05-18 19:43:37.656170 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the EmptyOperator and a custom EmptySkipOperator which skips by default.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport pendulum\n\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nif TYPE_CHECKING:\n    from airflow.utils.context import Context\n\n\n# Create some placeholder operators\nclass EmptySkipOperator(BaseOperator):\n    \"\"\"Empty operator which always skips the task.\"\"\"\n\n    ui_color = \"#e8b7e4\"\n\n    def execute(self, context: Context):\n        raise AirflowSkipException\n\n\ndef create_test_pipeline(suffix, trigger_rule):\n    \"\"\"\n    Instantiate a number of operators for the given DAG.\n\n    :param str suffix: Suffix to append to the operator task_ids\n    :param str trigger_rule: TriggerRule for the join task\n    :param DAG dag_: The DAG to run the operators on\n    \"\"\"\n    skip_operator = EmptySkipOperator(task_id=f\"skip_operator_{suffix}\")\n    always_true = EmptyOperator(task_id=f\"always_true_{suffix}\")\n    join = EmptyOperator(task_id=trigger_rule, trigger_rule=trigger_rule)\n    final = EmptyOperator(task_id=f\"final_{suffix}\")\n\n    skip_operator >> join\n    always_true >> join\n    join >> final\n\n\nwith DAG(\n    dag_id=\"example_skip_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    create_test_pipeline(\"1\", TriggerRule.ALL_SUCCESS)\n    create_test_pipeline(\"2\", TriggerRule.ONE_SUCCESS)\n"
  },
  {
    "fileloc_hash": 10807322491329423,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_sla_dag.py",
    "last_updated": "2024-05-18 19:43:37.656251 +00:00",
    "source_code": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating SLA use in Tasks\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport time\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\n\n\n# [START howto_task_sla]\ndef sla_callback(dag, task_list, blocking_task_list, slas, blocking_tis):\n    print(\n        \"The callback arguments are: \",\n        {\n            \"dag\": dag,\n            \"task_list\": task_list,\n            \"blocking_task_list\": blocking_task_list,\n            \"slas\": slas,\n            \"blocking_tis\": blocking_tis,\n        },\n    )\n\n\n@dag(\n    schedule=\"*/2 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    sla_miss_callback=sla_callback,\n    default_args={\"email\": \"email@example.com\"},\n)\ndef example_sla_dag():\n    @task(sla=datetime.timedelta(seconds=10))\n    def sleep_20():\n        \"\"\"Sleep for 20 seconds\"\"\"\n        time.sleep(20)\n\n    @task\n    def sleep_30():\n        \"\"\"Sleep for 30 seconds\"\"\"\n        time.sleep(30)\n\n    sleep_20() >> sleep_30()\n\n\nexample_dag = example_sla_dag()\n\n# [END howto_task_sla]\n"
  },
  {
    "fileloc_hash": 29266941926456311,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_task_group_decorator.py",
    "last_updated": "2024-05-18 19:43:37.657197 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the @taskgroup decorator.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task, task_group\nfrom airflow.models.dag import DAG\n\n\n# [START howto_task_group_decorator]\n# Creating Tasks\n@task\ndef task_start():\n    \"\"\"Empty Task which is First Task of Dag\"\"\"\n    return \"[Task_start]\"\n\n\n@task\ndef task_1(value: int) -> str:\n    \"\"\"Empty Task1\"\"\"\n    return f\"[ Task1 {value} ]\"\n\n\n@task\ndef task_2(value: str) -> str:\n    \"\"\"Empty Task2\"\"\"\n    return f\"[ Task2 {value} ]\"\n\n\n@task\ndef task_3(value: str) -> None:\n    \"\"\"Empty Task3\"\"\"\n    print(f\"[ Task3 {value} ]\")\n\n\n@task\ndef task_end() -> None:\n    \"\"\"Empty Task which is Last Task of Dag\"\"\"\n    print(\"[ Task_End  ]\")\n\n\n# Creating TaskGroups\n@task_group\ndef task_group_function(value: int) -> None:\n    \"\"\"TaskGroup for grouping related Tasks\"\"\"\n    task_3(task_2(task_1(value)))\n\n\n# Executing Tasks and TaskGroups\nwith DAG(\n    dag_id=\"example_task_group_decorator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start_task = task_start()\n    end_task = task_end()\n    for i in range(5):\n        current_task_group = task_group_function(i)\n        start_task >> current_task_group >> end_task\n\n# [END howto_task_group_decorator]\n"
  },
  {
    "fileloc_hash": 16322872089687030,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_dag_decorator.py",
    "last_updated": "2024-05-18 19:43:37.656407 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nimport httpx\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import BaseOperator\nfrom airflow.operators.email import EmailOperator\n\nif TYPE_CHECKING:\n    from airflow.utils.context import Context\n\n\nclass GetRequestOperator(BaseOperator):\n    \"\"\"Custom operator to send GET request to provided url\"\"\"\n\n    def __init__(self, *, url: str, **kwargs):\n        super().__init__(**kwargs)\n        self.url = url\n\n    def execute(self, context: Context):\n        return httpx.get(self.url).json()\n\n\n# [START dag_decorator_usage]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_dag_decorator(email: str = \"example@example.com\"):\n    \"\"\"\n    DAG to send server IP to email.\n\n    :param email: Email to send IP to. Defaults to example@example.com.\n    \"\"\"\n    get_ip = GetRequestOperator(task_id=\"get_ip\", url=\"http://httpbin.org/get\")\n\n    @task(multiple_outputs=True)\n    def prepare_email(raw_json: dict[str, Any]) -> dict[str, str]:\n        external_ip = raw_json[\"origin\"]\n        return {\n            \"subject\": f\"Server connected from {external_ip}\",\n            \"body\": f\"Seems like today your server executing Airflow is connected from IP {external_ip}<br>\",\n        }\n\n    email_info = prepare_email(get_ip.output)\n\n    EmailOperator(\n        task_id=\"send_email\", to=email, subject=email_info[\"subject\"], html_content=email_info[\"body\"]\n    )\n\n\nexample_dag = example_dag_decorator()\n# [END dag_decorator_usage]\n"
  },
  {
    "fileloc_hash": 5111331835162429,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_nested_branch_dag.py",
    "last_updated": "2024-05-18 19:43:37.656523 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating a workflow with nested branching. The join tasks are created with\n``none_failed_min_one_success`` trigger rule such that they are skipped whenever their corresponding\nbranching tasks are skipped.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"example_nested_branch_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@daily\",\n    tags=[\"example\"],\n) as dag:\n\n    @task.branch()\n    def branch(task_id_to_return: str) -> str:\n        return task_id_to_return\n\n    branch_1 = branch.override(task_id=\"branch_1\")(task_id_to_return=\"true_1\")\n    join_1 = EmptyOperator(task_id=\"join_1\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_1 = EmptyOperator(task_id=\"true_1\")\n    false_1 = EmptyOperator(task_id=\"false_1\")\n\n    branch_2 = branch.override(task_id=\"branch_2\")(task_id_to_return=\"true_2\")\n    join_2 = EmptyOperator(task_id=\"join_2\", trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS)\n    true_2 = EmptyOperator(task_id=\"true_2\")\n    false_2 = EmptyOperator(task_id=\"false_2\")\n    false_3 = EmptyOperator(task_id=\"false_3\")\n\n    branch_1 >> true_1 >> join_1\n    branch_1 >> false_1 >> branch_2 >> [true_2, false_2] >> join_2 >> false_3 >> join_1\n"
  },
  {
    "fileloc_hash": 22299540276451980,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_bash_operator.py",
    "last_updated": "2024-05-18 19:43:37.656625 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"example_bash_operator\",\n    schedule=\"0 0 * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=60),\n    tags=[\"example\", \"example2\"],\n    params={\"example_key\": \"example_value\"},\n) as dag:\n    run_this_last = EmptyOperator(\n        task_id=\"run_this_last\",\n    )\n\n    # [START howto_operator_bash]\n    run_this = BashOperator(\n        task_id=\"run_after_loop\",\n        bash_command=\"ls -alh --color=always / && echo https://airflow.apache.org/  && echo 'some <code>html</code>'\",\n    )\n    # [END howto_operator_bash]\n\n    run_this >> run_this_last\n\n    for i in range(3):\n        task = BashOperator(\n            task_id=f\"runme_{i}\",\n            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n        )\n        task >> run_this\n\n    # [START howto_operator_bash_template]\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command='echo \"ti_key={{ task_instance_key_str }}\"',\n    )\n    # [END howto_operator_bash_template]\n    also_run_this >> run_this_last\n\n# [START howto_operator_bash_skip]\nthis_will_skip = BashOperator(\n    task_id=\"this_will_skip\",\n    bash_command='echo \"hello world\"; exit 99;',\n    dag=dag,\n)\n# [END howto_operator_bash_skip]\nthis_will_skip >> run_this_last\n\nif __name__ == \"__main__\":\n    dag.test()\n"
  },
  {
    "fileloc_hash": 54529826502727696,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_subdag_operator.py",
    "last_updated": "2024-05-18 19:43:37.656729 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the SubDagOperator.\"\"\"\n\nfrom __future__ import annotations\n\n# [START example_subdag_operator]\nimport datetime\n\nfrom airflow.example_dags.subdags.subdag import subdag\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.subdag import SubDagOperator\n\nDAG_NAME = \"example_subdag_operator\"\n\nwith DAG(\n    dag_id=DAG_NAME,\n    default_args={\"retries\": 2},\n    start_date=datetime.datetime(2022, 1, 1),\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n    start = EmptyOperator(\n        task_id=\"start\",\n    )\n\n    section_1 = SubDagOperator(\n        task_id=\"section-1\",\n        subdag=subdag(DAG_NAME, \"section-1\", dag.default_args),\n    )\n\n    some_other_task = EmptyOperator(\n        task_id=\"some-other-task\",\n    )\n\n    section_2 = SubDagOperator(\n        task_id=\"section-2\",\n        subdag=subdag(DAG_NAME, \"section-2\", dag.default_args),\n    )\n\n    end = EmptyOperator(\n        task_id=\"end\",\n    )\n\n    start >> section_1 >> some_other_task >> section_2 >> end\n# [END example_subdag_operator]\n"
  },
  {
    "fileloc_hash": 20997191236416151,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_trigger_controller_dag.py",
    "last_updated": "2024-05-18 19:43:37.656836 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample usage of the TriggerDagRunOperator. This example holds 2 DAGs:\n1. 1st DAG (example_trigger_controller_dag) holds a TriggerDagRunOperator, which will trigger the 2nd DAG\n2. 2nd DAG (example_trigger_target_dag) which will be triggered by the TriggerDagRunOperator in the 1st DAG\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\nwith DAG(\n    dag_id=\"example_trigger_controller_dag\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n    trigger = TriggerDagRunOperator(\n        task_id=\"test_trigger_dagrun\",\n        trigger_dag_id=\"example_trigger_target_dag\",  # Ensure this equals the dag_id of the DAG to trigger\n        conf={\"message\": \"Hello World\"},\n    )\n"
  },
  {
    "fileloc_hash": 45159413034866623,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_latest_only_with_trigger.py",
    "last_updated": "2024-05-18 19:43:37.656930 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample LatestOnlyOperator and TriggerRule interactions\n\"\"\"\n\nfrom __future__ import annotations\n\n# [START example]\nimport datetime\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.latest_only import LatestOnlyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\nwith DAG(\n    dag_id=\"latest_only_with_trigger\",\n    schedule=datetime.timedelta(hours=4),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example3\"],\n) as dag:\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n    task3 = EmptyOperator(task_id=\"task3\")\n    task4 = EmptyOperator(task_id=\"task4\", trigger_rule=TriggerRule.ALL_DONE)\n\n    latest_only >> task1 >> [task3, task4]\n    task2 >> [task3, task4]\n# [END example]\n"
  },
  {
    "fileloc_hash": 55892031053790785,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py",
    "last_updated": "2024-05-18 19:43:37.657022 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating the usage of BranchDayOfWeekOperator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.weekday import BranchDayOfWeekOperator\nfrom airflow.utils.weekday import WeekDay\n\nwith DAG(\n    dag_id=\"example_weekday_branch_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n) as dag:\n    # [START howto_operator_day_of_week_branch]\n    empty_task_1 = EmptyOperator(task_id=\"branch_true\")\n    empty_task_2 = EmptyOperator(task_id=\"branch_false\")\n    empty_task_3 = EmptyOperator(task_id=\"branch_weekend\")\n    empty_task_4 = EmptyOperator(task_id=\"branch_mid_week\")\n\n    branch = BranchDayOfWeekOperator(\n        task_id=\"make_choice\",\n        follow_task_ids_if_true=\"branch_true\",\n        follow_task_ids_if_false=\"branch_false\",\n        week_day=\"Monday\",\n    )\n    branch_weekend = BranchDayOfWeekOperator(\n        task_id=\"make_weekend_choice\",\n        follow_task_ids_if_true=\"branch_weekend\",\n        follow_task_ids_if_false=\"branch_mid_week\",\n        week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},\n    )\n\n    # Run empty_task_1 if branch executes on Monday, empty_task_2 otherwise\n    branch >> [empty_task_1, empty_task_2]\n    # Run empty_task_3 if it's a weekend, empty_task_4 otherwise\n    empty_task_2 >> branch_weekend >> [empty_task_3, empty_task_4]\n    # [END howto_operator_day_of_week_branch]\n"
  },
  {
    "fileloc_hash": 30761950363703674,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_xcom.py",
    "last_updated": "2024-05-18 19:43:37.657113 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of XComs.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.models.xcom_arg import XComArg\nfrom airflow.operators.bash import BashOperator\n\nvalue_1 = [1, 2, 3]\nvalue_2 = {\"a\": \"b\"}\n\n\n@task\ndef push(ti=None):\n    \"\"\"Pushes an XCom without a specific target\"\"\"\n    ti.xcom_push(key=\"value from pusher 1\", value=value_1)\n\n\n@task\ndef push_by_returning():\n    \"\"\"Pushes an XCom without a specific target, just by returning it\"\"\"\n    return value_2\n\n\ndef _compare_values(pulled_value, check_value):\n    if pulled_value != check_value:\n        raise ValueError(f\"The two values differ {pulled_value} and {check_value}\")\n\n\n@task\ndef puller(pulled_value_2, ti=None):\n    \"\"\"Pull all previously pushed XComs and check if the pushed values match the pulled values.\"\"\"\n    pulled_value_1 = ti.xcom_pull(task_ids=\"push\", key=\"value from pusher 1\")\n\n    _compare_values(pulled_value_1, value_1)\n    _compare_values(pulled_value_2, value_2)\n\n\n@task\ndef pull_value_from_bash_push(ti=None):\n    bash_pushed_via_return_value = ti.xcom_pull(key=\"return_value\", task_ids=\"bash_push\")\n    bash_manually_pushed_value = ti.xcom_pull(key=\"manually_pushed_value\", task_ids=\"bash_push\")\n    print(f\"The xcom value pushed by task push via return value is {bash_pushed_via_return_value}\")\n    print(f\"The xcom value pushed by task push manually is {bash_manually_pushed_value}\")\n\n\nwith DAG(\n    \"example_xcom\",\n    schedule=\"@once\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    bash_push = BashOperator(\n        task_id=\"bash_push\",\n        bash_command='echo \"bash_push demo\"  && '\n        'echo \"Manually set xcom value '\n        '{{ ti.xcom_push(key=\"manually_pushed_value\", value=\"manually_pushed_value\") }}\" && '\n        'echo \"value_by_return\"',\n    )\n\n    bash_pull = BashOperator(\n        task_id=\"bash_pull\",\n        bash_command='echo \"bash pull demo\" && '\n        f'echo \"The xcom pushed manually is {XComArg(bash_push, key=\"manually_pushed_value\")}\" && '\n        f'echo \"The returned_value xcom is {XComArg(bash_push)}\" && '\n        'echo \"finished\"',\n        do_xcom_push=False,\n    )\n\n    python_pull_from_bash = pull_value_from_bash_push()\n\n    [bash_pull, python_pull_from_bash] << bash_push\n\n    puller(push_by_returning()) << push()\n"
  },
  {
    "fileloc_hash": 29195785422014505,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_xcomargs.py",
    "last_updated": "2024-05-18 19:43:37.657281 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the XComArgs.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\nlog = logging.getLogger(__name__)\n\n\n@task\ndef generate_value():\n    \"\"\"Empty function\"\"\"\n    return \"Bring me a shrubbery!\"\n\n\n@task\ndef print_value(value, ts=None):\n    \"\"\"Empty function\"\"\"\n    log.info(\"The knights of Ni say: %s (at %s)\", value, ts)\n\n\nwith DAG(\n    dag_id=\"example_xcom_args\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag:\n    print_value(generate_value())\n\nwith DAG(\n    \"example_xcom_args_with_operators\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    schedule=None,\n    tags=[\"example\"],\n) as dag2:\n    bash_op1 = BashOperator(task_id=\"c\", bash_command=\"echo c\")\n    bash_op2 = BashOperator(task_id=\"d\", bash_command=\"echo c\")\n    xcom_args_a = print_value(\"first!\")\n    xcom_args_b = print_value(\"second!\")\n\n    bash_op1 >> xcom_args_a >> xcom_args_b >> bash_op2\n"
  },
  {
    "fileloc_hash": 6991602709552801,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_time_delta_sensor_async.py",
    "last_updated": "2024-05-18 19:43:37.657388 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating ``TimeDeltaSensorAsync``, a drop in replacement for ``TimeDeltaSensor`` that\ndefers and doesn't occupy a worker slot while it waits\n\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.sensors.time_delta import TimeDeltaSensorAsync\n\nwith DAG(\n    dag_id=\"example_time_delta_sensor_async\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    wait = TimeDeltaSensorAsync(task_id=\"wait\", delta=datetime.timedelta(seconds=30))\n    finish = EmptyOperator(task_id=\"finish\")\n    wait >> finish\n"
  },
  {
    "fileloc_hash": 69981351927492411,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_passing_params_via_test_command.py",
    "last_updated": "2024-05-18 19:43:37.657492 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the params arguments in templated arguments.\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport os\nimport textwrap\n\nimport pendulum\n\nfrom airflow.decorators import task\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\n\n\n@task(task_id=\"run_this\")\ndef my_py_command(params, test_mode=None, task=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command run_this <date>\n    -t '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(\n            f\" 'foo' was passed in via test={test_mode} command : kwargs[params][foo] = {task.params['foo']}\"\n        )\n    # Print out the value of \"miff\", passed in below via the Python Operator\n    print(f\" 'miff' was passed in via task params = {params['miff']}\")\n    return 1\n\n\n@task(task_id=\"env_var_test_task\")\ndef print_env_vars(test_mode=None):\n    \"\"\"\n    Print out the \"foo\" param passed in via\n    `airflow tasks test example_passing_params_via_test_command env_var_test_task <date>\n    --env-vars '{\"foo\":\"bar\"}'`\n    \"\"\"\n    if test_mode:\n        print(f\"foo={os.environ.get('foo')}\")\n        print(f\"AIRFLOW_TEST_MODE={os.environ.get('AIRFLOW_TEST_MODE')}\")\n\n\nwith DAG(\n    \"example_passing_params_via_test_command\",\n    schedule=\"*/1 * * * *\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=4),\n    tags=[\"example\"],\n) as dag:\n    run_this = my_py_command(params={\"miff\": \"agg\"})\n\n    my_command = textwrap.dedent(\n        \"\"\"\n        echo \"'foo' was passed in via Airflow CLI Test command with value '$FOO'\"\n        echo \"'miff' was passed in via BashOperator with value '$MIFF'\"\n        \"\"\"\n    )\n\n    also_run_this = BashOperator(\n        task_id=\"also_run_this\",\n        bash_command=my_command,\n        params={\"miff\": \"agg\"},\n        env={\"FOO\": \"{{ params.foo }}\", \"MIFF\": \"{{ params.miff }}\"},\n    )\n\n    env_var_test_task = print_env_vars()\n\n    run_this >> also_run_this\n"
  },
  {
    "fileloc_hash": 19275888862540794,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py",
    "last_updated": "2024-05-18 19:43:37.657590 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of setup and teardown tasks.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import setup, task, task_group, teardown\nfrom airflow.models.dag import DAG\n\nwith DAG(\n    dag_id=\"example_setup_teardown_taskflow\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    @task\n    def my_first_task():\n        print(\"Hello 1\")\n\n    @task\n    def my_second_task():\n        print(\"Hello 2\")\n\n    @task\n    def my_third_task():\n        print(\"Hello 3\")\n\n    # you can set setup / teardown relationships with the `as_teardown` method.\n    task_1 = my_first_task()\n    task_2 = my_second_task()\n    task_3 = my_third_task()\n    task_1 >> task_2 >> task_3.as_teardown(setups=task_1)\n\n    # The method `as_teardown` will mark task_3 as teardown, task_1 as setup, and\n    # arrow task_1 >> task_3.\n    # Now if you clear task_2, then its setup task, task_1, will be cleared in\n    # addition to its teardown task, task_3\n\n    # it's also possible to use a decorator to mark a task as setup or\n    # teardown when you define it. see below.\n\n    @setup\n    def outer_setup():\n        print(\"I am outer_setup\")\n        return \"some cluster id\"\n\n    @teardown\n    def outer_teardown(cluster_id):\n        print(\"I am outer_teardown\")\n        print(f\"Tearing down cluster: {cluster_id}\")\n\n    @task\n    def outer_work():\n        print(\"I am just a normal task\")\n\n    @task_group\n    def section_1():\n        @setup\n        def inner_setup():\n            print(\"I set up\")\n            return \"some_cluster_id\"\n\n        @task\n        def inner_work(cluster_id):\n            print(f\"doing some work with {cluster_id=}\")\n\n        @teardown\n        def inner_teardown(cluster_id):\n            print(f\"tearing down {cluster_id=}\")\n\n        # this passes the return value of `inner_setup` to both `inner_work` and `inner_teardown`\n        inner_setup_task = inner_setup()\n        inner_work(inner_setup_task) >> inner_teardown(inner_setup_task)\n\n    # by using the decorators, outer_setup and outer_teardown are already marked as setup / teardown\n    # now we just need to make sure they are linked directly.  At a low level, what we need\n    # to do so is the following::\n    #     s = outer_setup()\n    #     t = outer_teardown()\n    #     s >> t\n    #     s >> outer_work() >> t\n    # Thus, s and t are linked directly, and outer_work runs in between.  We can take advantage of\n    # the fact that we are in taskflow, along with the context manager on teardowns, as follows:\n    with outer_teardown(outer_setup()):\n        outer_work()\n\n        # and let's put section 1 inside the outer setup and teardown tasks\n        section_1()\n"
  },
  {
    "fileloc_hash": 69030026274976066,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_bash_decorator.py",
    "last_updated": "2024-05-18 19:43:37.657692 +00:00",
    "source_code": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\n\n@dag(schedule=None, start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"), catchup=False)\ndef example_bash_decorator():\n    @task.bash\n    def run_me(sleep_seconds: int, task_instance_key_str: str) -> str:\n        return f\"echo {task_instance_key_str} && sleep {sleep_seconds}\"\n\n    run_me_loop = [run_me.override(task_id=f\"runme_{i}\")(sleep_seconds=i) for i in range(3)]\n\n    # [START howto_decorator_bash]\n    @task.bash\n    def run_after_loop() -> str:\n        return \"echo 1\"\n\n    run_this = run_after_loop()\n    # [END howto_decorator_bash]\n\n    # [START howto_decorator_bash_template]\n    @task.bash\n    def also_run_this() -> str:\n        return 'echo \"ti_key={{ task_instance_key_str }}\"'\n\n    also_this = also_run_this()\n    # [END howto_decorator_bash_template]\n\n    # [START howto_decorator_bash_context_vars]\n    @task.bash\n    def also_run_this_again(task_instance_key_str) -> str:\n        return f'echo \"ti_key={task_instance_key_str}\"'\n\n    also_this_again = also_run_this_again()\n    # [END howto_decorator_bash_context_vars]\n\n    # [START howto_decorator_bash_skip]\n    @task.bash\n    def this_will_skip() -> str:\n        return 'echo \"hello world\"; exit 99;'\n\n    this_skips = this_will_skip()\n    # [END howto_decorator_bash_skip]\n\n    run_this_last = EmptyOperator(task_id=\"run_this_last\", trigger_rule=TriggerRule.ALL_DONE)\n\n    # [START howto_decorator_bash_conditional]\n    @task.bash\n    def sleep_in(day: str) -> str:\n        if day in (WeekDay.SATURDAY, WeekDay.SUNDAY):\n            return f\"sleep {60 * 60}\"\n        else:\n            raise AirflowSkipException(\"No sleeping in today!\")\n\n    sleep_in(day=\"{{ dag_run.logical_date.strftime('%A').lower() }}\")\n    # [END howto_decorator_bash_conditional]\n\n    # [START howto_decorator_bash_parametrize]\n    @task.bash(env={\"BASE_DIR\": \"{{ dag_run.logical_date.strftime('%Y/%m/%d') }}\"}, append_env=True)\n    def make_dynamic_dirs(new_dirs: str) -> str:\n        return f\"mkdir -p $AIRFLOW_HOME/$BASE_DIR/{new_dirs}\"\n\n    make_dynamic_dirs(new_dirs=\"foo/bar/baz\")\n    # [END howto_decorator_bash_parametrize]\n\n    # [START howto_decorator_bash_build_cmd]\n    def _get_files_in_cwd() -> list[str]:\n        from pathlib import Path\n\n        dir_contents = Path.cwd().glob(\"airflow/example_dags/*.py\")\n        files = [str(elem) for elem in dir_contents if elem.is_file()]\n\n        return files\n\n    @task.bash\n    def get_file_stats() -> str:\n        files = _get_files_in_cwd()\n        cmd = \"stat \"\n        cmd += \" \".join(files)\n\n        return cmd\n\n    get_file_stats()\n    # [END howto_decorator_bash_build_cmd]\n\n    chain(run_me_loop, run_this)\n    chain([also_this, also_this_again, this_skips, run_this], run_this_last)\n\n\nexample_bash_decorator()\n"
  },
  {
    "fileloc_hash": 41726306565972796,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_branch_datetime_operator.py",
    "last_updated": "2024-05-18 19:43:37.657783 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating the usage of DateTimeBranchOperator with datetime as well as time objects as\ntargets.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.datetime import BranchDateTimeOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndag1 = DAG(\n    dag_id=\"example_branch_datetime_operator\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n\n# [START howto_branch_datetime_operator]\nempty_task_11 = EmptyOperator(task_id=\"date_in_range\", dag=dag1)\nempty_task_21 = EmptyOperator(task_id=\"date_outside_range\", dag=dag1)\n\ncond1 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag1,\n)\n\n# Run empty_task_11 if cond1 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond1 >> [empty_task_11, empty_task_21]\n# [END howto_branch_datetime_operator]\n\n\ndag2 = DAG(\n    dag_id=\"example_branch_datetime_operator_2\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_next_day]\nempty_task_12 = EmptyOperator(task_id=\"date_in_range\", dag=dag2)\nempty_task_22 = EmptyOperator(task_id=\"date_outside_range\", dag=dag2)\n\ncond2 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.time(0, 0, 0),\n    target_lower=pendulum.time(15, 0, 0),\n    dag=dag2,\n)\n\n# Since target_lower happens after target_upper, target_upper will be moved to the following day\n# Run empty_task_12 if cond2 executes between 15:00:00, and 00:00:00 of the following day\ncond2 >> [empty_task_12, empty_task_22]\n# [END howto_branch_datetime_operator_next_day]\n\ndag3 = DAG(\n    dag_id=\"example_branch_datetime_operator_3\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n    schedule=\"@daily\",\n)\n# [START howto_branch_datetime_operator_logical_date]\nempty_task_13 = EmptyOperator(task_id=\"date_in_range\", dag=dag3)\nempty_task_23 = EmptyOperator(task_id=\"date_outside_range\", dag=dag3)\n\ncond3 = BranchDateTimeOperator(\n    task_id=\"datetime_branch\",\n    use_task_logical_date=True,\n    follow_task_ids_if_true=[\"date_in_range\"],\n    follow_task_ids_if_false=[\"date_outside_range\"],\n    target_upper=pendulum.datetime(2020, 10, 10, 15, 0, 0),\n    target_lower=pendulum.datetime(2020, 10, 10, 14, 0, 0),\n    dag=dag3,\n)\n\n# Run empty_task_13 if cond3 executes between 2020-10-10 14:00:00 and 2020-10-10 15:00:00\ncond3 >> [empty_task_13, empty_task_23]\n# [END howto_branch_datetime_operator_logical_date]\n"
  },
  {
    "fileloc_hash": 35550515220461510,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_branch_labels.py",
    "last_updated": "2024-05-18 19:43:37.657869 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating the usage of labels with different branches.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\n\nwith DAG(\n    \"example_branch_labels\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n) as dag:\n    ingest = EmptyOperator(task_id=\"ingest\")\n    analyse = EmptyOperator(task_id=\"analyze\")\n    check = EmptyOperator(task_id=\"check_integrity\")\n    describe = EmptyOperator(task_id=\"describe_integrity\")\n    error = EmptyOperator(task_id=\"email_error\")\n    save = EmptyOperator(task_id=\"save\")\n    report = EmptyOperator(task_id=\"report\")\n\n    ingest >> analyse >> check\n    check >> Label(\"No errors\") >> save >> report\n    check >> Label(\"Errors found\") >> describe >> error >> report\n"
  },
  {
    "fileloc_hash": 71447668620450815,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_sensor_decorator.py",
    "last_updated": "2024-05-18 19:43:37.657952 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Example DAG demonstrating the usage of the sensor decorator.\"\"\"\n\nfrom __future__ import annotations\n\n# [START tutorial]\n# [START import_module]\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.sensors.base import PokeReturnValue\n\n# [END import_module]\n\n\n# [START instantiate_dag]\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef example_sensor_decorator():\n    # [END instantiate_dag]\n\n    # [START wait_function]\n    # Using a sensor operator to wait for the upstream data to be ready.\n    @task.sensor(poke_interval=60, timeout=3600, mode=\"reschedule\")\n    def wait_for_upstream() -> PokeReturnValue:\n        return PokeReturnValue(is_done=True, xcom_value=\"xcom_value\")\n\n    # [END wait_function]\n\n    # [START dummy_function]\n    @task\n    def dummy_operator() -> None:\n        pass\n\n    # [END dummy_function]\n\n    # [START main_flow]\n    wait_for_upstream() >> dummy_operator()\n    # [END main_flow]\n\n\n# [START dag_invocation]\ntutorial_etl_dag = example_sensor_decorator()\n# [END dag_invocation]\n\n# [END tutorial]\n"
  },
  {
    "fileloc_hash": 34062846983196796,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_python_operator.py",
    "last_updated": "2024-05-18 19:43:37.658043 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nExample DAG demonstrating the usage of the classic Python operators to execute Python functions natively and\nwithin a virtual environment.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport sys\nimport time\nfrom pprint import pprint\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.python import (\n    ExternalPythonOperator,\n    PythonOperator,\n    PythonVirtualenvOperator,\n    is_venv_installed,\n)\n\nlog = logging.getLogger(__name__)\n\nPATH_TO_PYTHON_BINARY = sys.executable\n\n\nwith DAG(\n    dag_id=\"example_python_operator\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n):\n    # [START howto_operator_python]\n    def print_context(ds=None, **kwargs):\n        \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n        print(\"::group::All kwargs\")\n        pprint(kwargs)\n        print(\"::endgroup::\")\n        print(\"::group::Context variable ds\")\n        print(ds)\n        print(\"::endgroup::\")\n        return \"Whatever you return gets printed in the logs\"\n\n    run_this = PythonOperator(task_id=\"print_the_context\", python_callable=print_context)\n    # [END howto_operator_python]\n\n    # [START howto_operator_python_render_sql]\n    def log_sql(**kwargs):\n        log.info(\"Python task decorator query: %s\", str(kwargs[\"templates_dict\"][\"query\"]))\n\n    log_the_sql = PythonOperator(\n        task_id=\"log_sql_query\",\n        python_callable=log_sql,\n        templates_dict={\"query\": \"sql/sample.sql\"},\n        templates_exts=[\".sql\"],\n    )\n    # [END howto_operator_python_render_sql]\n\n    # [START howto_operator_python_kwargs]\n    # Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively\n    def my_sleeping_function(random_base):\n        \"\"\"This is a function that will run within the DAG execution\"\"\"\n        time.sleep(random_base)\n\n    for i in range(5):\n        sleeping_task = PythonOperator(\n            task_id=f\"sleep_for_{i}\", python_callable=my_sleeping_function, op_kwargs={\"random_base\": i / 10}\n        )\n\n        run_this >> log_the_sql >> sleeping_task\n    # [END howto_operator_python_kwargs]\n\n    if not is_venv_installed():\n        log.warning(\"The virtalenv_python example task requires virtualenv, please install it.\")\n    else:\n        # [START howto_operator_python_venv]\n        def callable_virtualenv():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            from time import sleep\n\n            from colorama import Back, Fore, Style\n\n            print(Fore.RED + \"some red text\")\n            print(Back.GREEN + \"and with a green background\")\n            print(Style.DIM + \"and in dim text\")\n            print(Style.RESET_ALL)\n            for _ in range(4):\n                print(Style.DIM + \"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        virtualenv_task = PythonVirtualenvOperator(\n            task_id=\"virtualenv_python\",\n            python_callable=callable_virtualenv,\n            requirements=[\"colorama==0.4.0\"],\n            system_site_packages=False,\n        )\n        # [END howto_operator_python_venv]\n\n        sleeping_task >> virtualenv_task\n\n        # [START howto_operator_external_python]\n        def callable_external_python():\n            \"\"\"\n            Example function that will be performed in a virtual environment.\n\n            Importing at the module level ensures that it will not attempt to import the\n            library before it is installed.\n            \"\"\"\n            import sys\n            from time import sleep\n\n            print(f\"Running task via {sys.executable}\")\n            print(\"Sleeping\")\n            for _ in range(4):\n                print(\"Please wait...\", flush=True)\n                sleep(1)\n            print(\"Finished\")\n\n        external_python_task = ExternalPythonOperator(\n            task_id=\"external_python\",\n            python_callable=callable_external_python,\n            python=PATH_TO_PYTHON_BINARY,\n        )\n        # [END howto_operator_external_python]\n\n        run_this >> external_python_task >> virtualenv_task\n"
  },
  {
    "fileloc_hash": 19642424241510907,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_sensors.py",
    "last_updated": "2024-05-18 19:43:37.658130 +00:00",
    "source_code": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\n\nimport datetime\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.sensors.bash import BashSensor\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.sensors.python import PythonSensor\nfrom airflow.sensors.time_delta import TimeDeltaSensor, TimeDeltaSensorAsync\nfrom airflow.sensors.time_sensor import TimeSensor, TimeSensorAsync\nfrom airflow.sensors.weekday import DayOfWeekSensor\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.weekday import WeekDay\n\n\n# [START example_callables]\ndef success_callable():\n    return True\n\n\ndef failure_callable():\n    return False\n\n\n# [END example_callables]\n\n\nwith DAG(\n    dag_id=\"example_sensors\",\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # [START example_time_delta_sensor]\n    t0 = TimeDeltaSensor(task_id=\"wait_some_seconds\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor]\n\n    # [START example_time_delta_sensor_async]\n    t0a = TimeDeltaSensorAsync(task_id=\"wait_some_seconds_async\", delta=datetime.timedelta(seconds=2))\n    # [END example_time_delta_sensor_async]\n\n    # [START example_time_sensors]\n    t1 = TimeSensor(\n        task_id=\"fire_immediately\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2 = TimeSensor(\n        task_id=\"timeout_after_second_date_in_the_future\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors]\n\n    # [START example_time_sensors_async]\n    t1a = TimeSensorAsync(\n        task_id=\"fire_immediately_async\", target_time=datetime.datetime.now(tz=datetime.timezone.utc).time()\n    )\n\n    t2a = TimeSensorAsync(\n        task_id=\"timeout_after_second_date_in_the_future_async\",\n        timeout=1,\n        soft_fail=True,\n        target_time=(datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(hours=1)).time(),\n    )\n    # [END example_time_sensors_async]\n\n    # [START example_bash_sensors]\n    t3 = BashSensor(task_id=\"Sensor_succeeds\", bash_command=\"exit 0\")\n\n    t4 = BashSensor(task_id=\"Sensor_fails_after_3_seconds\", timeout=3, soft_fail=True, bash_command=\"exit 1\")\n    # [END example_bash_sensors]\n\n    t5 = BashOperator(task_id=\"remove_file\", bash_command=\"rm -rf /tmp/temporary_file_for_testing\")\n\n    # [START example_file_sensor]\n    t6 = FileSensor(task_id=\"wait_for_file\", filepath=\"/tmp/temporary_file_for_testing\")\n    # [END example_file_sensor]\n\n    # [START example_file_sensor_async]\n    t7 = FileSensor(\n        task_id=\"wait_for_file_async\", filepath=\"/tmp/temporary_file_for_testing\", deferrable=True\n    )\n    # [END example_file_sensor_async]\n\n    t8 = BashOperator(\n        task_id=\"create_file_after_3_seconds\", bash_command=\"sleep 3; touch /tmp/temporary_file_for_testing\"\n    )\n\n    # [START example_python_sensors]\n    t9 = PythonSensor(task_id=\"success_sensor_python\", python_callable=success_callable)\n\n    t10 = PythonSensor(\n        task_id=\"failure_timeout_sensor_python\", timeout=3, soft_fail=True, python_callable=failure_callable\n    )\n    # [END example_python_sensors]\n\n    # [START example_day_of_week_sensor]\n    t11 = DayOfWeekSensor(\n        task_id=\"week_day_sensor_failing_on_timeout\", timeout=3, soft_fail=True, week_day=WeekDay.MONDAY\n    )\n    # [END example_day_of_week_sensor]\n\n    tx = BashOperator(task_id=\"print_date_in_bash\", bash_command=\"date\")\n\n    tx.trigger_rule = TriggerRule.NONE_FAILED\n    [t0, t0a, t1, t1a, t2, t2a, t3, t4] >> tx\n    t5 >> t6 >> t7 >> tx\n    t8 >> tx\n    [t9, t10] >> tx\n    t11 >> tx\n"
  },
  {
    "fileloc_hash": 9020322650562930,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_task_group.py",
    "last_updated": "2024-05-18 19:43:37.658213 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the TaskGroup.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.models.dag import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.task_group import TaskGroup\n\n# [START howto_task_group]\nwith DAG(\n    dag_id=\"example_task_group\",\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    start = EmptyOperator(task_id=\"start\")\n\n    # [START howto_task_group_section_1]\n    with TaskGroup(\"section_1\", tooltip=\"Tasks for section_1\") as section_1:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n        task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n        task_3 = EmptyOperator(task_id=\"task_3\")\n\n        task_1 >> [task_2, task_3]\n    # [END howto_task_group_section_1]\n\n    # [START howto_task_group_section_2]\n    with TaskGroup(\"section_2\", tooltip=\"Tasks for section_2\") as section_2:\n        task_1 = EmptyOperator(task_id=\"task_1\")\n\n        # [START howto_task_group_inner_section_2]\n        with TaskGroup(\"inner_section_2\", tooltip=\"Tasks for inner_section2\") as inner_section_2:\n            task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 1\")\n            task_3 = EmptyOperator(task_id=\"task_3\")\n            task_4 = EmptyOperator(task_id=\"task_4\")\n\n            [task_2, task_3] >> task_4\n        # [END howto_task_group_inner_section_2]\n\n    # [END howto_task_group_section_2]\n\n    end = EmptyOperator(task_id=\"end\")\n\n    start >> section_1 >> section_2 >> end\n# [END howto_task_group]\n"
  },
  {
    "fileloc_hash": 53987879009019060,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_short_circuit_decorator.py",
    "last_updated": "2024-05-18 19:43:37.658301 +00:00",
    "source_code": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Example DAG demonstrating the usage of the `@task.short_circuit()` TaskFlow decorator.\"\"\"\n\nfrom __future__ import annotations\n\nimport pendulum\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.trigger_rule import TriggerRule\n\n\n@dag(start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"), catchup=False, tags=[\"example\"])\ndef example_short_circuit_decorator():\n    # [START howto_operator_short_circuit]\n    @task.short_circuit()\n    def check_condition(condition):\n        return condition\n\n    ds_true = [EmptyOperator(task_id=f\"true_{i}\") for i in [1, 2]]\n    ds_false = [EmptyOperator(task_id=f\"false_{i}\") for i in [1, 2]]\n\n    condition_is_true = check_condition.override(task_id=\"condition_is_true\")(condition=True)\n    condition_is_false = check_condition.override(task_id=\"condition_is_false\")(condition=False)\n\n    chain(condition_is_true, *ds_true)\n    chain(condition_is_false, *ds_false)\n    # [END howto_operator_short_circuit]\n\n    # [START howto_operator_short_circuit_trigger_rules]\n    [task_1, task_2, task_3, task_4, task_5, task_6] = [\n        EmptyOperator(task_id=f\"task_{i}\") for i in range(1, 7)\n    ]\n\n    task_7 = EmptyOperator(task_id=\"task_7\", trigger_rule=TriggerRule.ALL_DONE)\n\n    short_circuit = check_condition.override(task_id=\"short_circuit\", ignore_downstream_trigger_rules=False)(\n        condition=False\n    )\n\n    chain(task_1, [task_2, short_circuit], [task_3, task_4], [task_5, task_6], task_7)\n    # [END howto_operator_short_circuit_trigger_rules]\n\n\nexample_dag = example_short_circuit_decorator()\n"
  },
  {
    "fileloc_hash": 28371954831505008,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py",
    "last_updated": "2024-05-18 19:59:31.958884 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nThis is an example dag for using a Local Kubernetes Executor Configuration.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom datetime import datetime\n\nfrom airflow.configuration import conf\nfrom airflow.decorators import task\nfrom airflow.example_dags.libs.helper import print_stuff\nfrom airflow.models.dag import DAG\n\nlog = logging.getLogger(__name__)\n\nworker_container_repository = conf.get(\"kubernetes_executor\", \"worker_container_repository\")\nworker_container_tag = conf.get(\"kubernetes_executor\", \"worker_container_tag\")\n\ntry:\n    from kubernetes.client import models as k8s\nexcept ImportError:\n    log.warning(\"Could not import DAGs in example_local_kubernetes_executor.py\", exc_info=True)\n    log.warning(\"Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\")\n    k8s = None\n\nif k8s:\n    with DAG(\n        dag_id=\"example_local_kubernetes_executor\",\n        schedule=None,\n        start_date=datetime(2021, 1, 1),\n        catchup=False,\n        tags=[\"example3\"],\n    ) as dag:\n        # You can use annotations on your kubernetes pods!\n        start_task_executor_config = {\n            \"pod_override\": k8s.V1Pod(metadata=k8s.V1ObjectMeta(annotations={\"test\": \"annotation\"}))\n        }\n\n        @task(\n            executor_config=start_task_executor_config,\n            queue=\"kubernetes\",\n            task_id=\"task_with_kubernetes_executor\",\n        )\n        def task_with_template():\n            print_stuff()\n\n        @task(task_id=\"task_with_local_executor\")\n        def task_with_local(ds=None, **kwargs):\n            \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n            print(kwargs)\n            print(ds)\n            return \"Whatever you return gets printed in the logs\"\n\n        task_with_local() >> task_with_template()\n"
  },
  {
    "fileloc_hash": 55605948998466395,
    "fileloc": "/opt/airflow/.local/lib/python3.8/site-packages/airflow/example_dags/example_kubernetes_executor.py",
    "last_updated": "2024-05-18 19:59:33.590717 +00:00",
    "source_code": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nThis is an example dag for using a Kubernetes Executor Configuration.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport os\n\nimport pendulum\n\nfrom airflow.configuration import conf\nfrom airflow.decorators import task\nfrom airflow.example_dags.libs.helper import print_stuff\nfrom airflow.models.dag import DAG\n\nlog = logging.getLogger(__name__)\n\ntry:\n    from kubernetes.client import models as k8s\nexcept ImportError:\n    log.warning(\n        \"The example_kubernetes_executor example DAG requires the kubernetes provider.\"\n        \" Please install it with: pip install apache-airflow[cncf.kubernetes]\"\n    )\n    k8s = None\n\n\nif k8s:\n    with DAG(\n        dag_id=\"example_kubernetes_executor\",\n        schedule=None,\n        start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n        catchup=False,\n        tags=[\"example3\"],\n    ) as dag:\n        # You can use annotations on your kubernetes pods!\n        start_task_executor_config = {\n            \"pod_override\": k8s.V1Pod(metadata=k8s.V1ObjectMeta(annotations={\"test\": \"annotation\"}))\n        }\n\n        @task(executor_config=start_task_executor_config)\n        def start_task():\n            print_stuff()\n\n        # [START task_with_volume]\n        executor_config_volume_mount = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\",\n                            volume_mounts=[\n                                k8s.V1VolumeMount(mount_path=\"/foo/\", name=\"example-kubernetes-test-volume\")\n                            ],\n                        )\n                    ],\n                    volumes=[\n                        k8s.V1Volume(\n                            name=\"example-kubernetes-test-volume\",\n                            host_path=k8s.V1HostPathVolumeSource(path=\"/tmp/\"),\n                        )\n                    ],\n                )\n            ),\n        }\n\n        @task(executor_config=executor_config_volume_mount)\n        def test_volume_mount():\n            \"\"\"\n            Tests whether the volume has been mounted.\n            \"\"\"\n\n            with open(\"/foo/volume_mount_test.txt\", \"w\") as foo:\n                foo.write(\"Hello\")\n\n            return_code = os.system(\"cat /foo/volume_mount_test.txt\")\n            if return_code != 0:\n                raise ValueError(f\"Error when checking volume mount. Return code {return_code}\")\n\n        volume_task = test_volume_mount()\n        # [END task_with_volume]\n\n        # [START task_with_sidecar]\n        executor_config_sidecar = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\",\n                            volume_mounts=[k8s.V1VolumeMount(mount_path=\"/shared/\", name=\"shared-empty-dir\")],\n                        ),\n                        k8s.V1Container(\n                            name=\"sidecar\",\n                            image=\"ubuntu\",\n                            args=['echo \"retrieved from mount\" > /shared/test.txt'],\n                            command=[\"bash\", \"-cx\"],\n                            volume_mounts=[k8s.V1VolumeMount(mount_path=\"/shared/\", name=\"shared-empty-dir\")],\n                        ),\n                    ],\n                    volumes=[\n                        k8s.V1Volume(name=\"shared-empty-dir\", empty_dir=k8s.V1EmptyDirVolumeSource()),\n                    ],\n                )\n            ),\n        }\n\n        @task(executor_config=executor_config_sidecar)\n        def test_sharedvolume_mount():\n            \"\"\"\n            Tests whether the volume has been mounted.\n            \"\"\"\n            for i in range(5):\n                try:\n                    return_code = os.system(\"cat /shared/test.txt\")\n                    if return_code != 0:\n                        raise ValueError(f\"Error when checking volume mount. Return code {return_code}\")\n                except ValueError as e:\n                    if i > 4:\n                        raise e\n\n        sidecar_task = test_sharedvolume_mount()\n        # [END task_with_sidecar]\n\n        # You can add labels to pods\n        executor_config_non_root = {\n            \"pod_override\": k8s.V1Pod(metadata=k8s.V1ObjectMeta(labels={\"release\": \"stable\"}))\n        }\n\n        @task(executor_config=executor_config_non_root)\n        def non_root_task():\n            print_stuff()\n\n        third_task = non_root_task()\n\n        executor_config_other_ns = {\n            \"pod_override\": k8s.V1Pod(\n                metadata=k8s.V1ObjectMeta(namespace=\"test-namespace\", labels={\"release\": \"stable\"})\n            )\n        }\n\n        @task(executor_config=executor_config_other_ns)\n        def other_namespace_task():\n            print_stuff()\n\n        other_ns_task = other_namespace_task()\n        worker_container_repository = conf.get(\"kubernetes_executor\", \"worker_container_repository\")\n        worker_container_tag = conf.get(\"kubernetes_executor\", \"worker_container_tag\")\n\n        # You can also change the base image, here we used the worker image for demonstration.\n        # Note that the image must have the same configuration as the\n        # worker image. Could be that you want to run this task in a special docker image that has a zip\n        # library built-in. You build the special docker image on top your worker image.\n        kube_exec_config_special = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\", image=f\"{worker_container_repository}:{worker_container_tag}\"\n                        ),\n                    ]\n                )\n            )\n        }\n\n        @task(executor_config=kube_exec_config_special)\n        def base_image_override_task():\n            print_stuff()\n\n        base_image_task = base_image_override_task()\n\n        # Use k8s_client.V1Affinity to define node affinity\n        k8s_affinity = k8s.V1Affinity(\n            pod_anti_affinity=k8s.V1PodAntiAffinity(\n                required_during_scheduling_ignored_during_execution=[\n                    k8s.V1PodAffinityTerm(\n                        label_selector=k8s.V1LabelSelector(\n                            match_expressions=[\n                                k8s.V1LabelSelectorRequirement(key=\"app\", operator=\"In\", values=[\"airflow\"])\n                            ]\n                        ),\n                        topology_key=\"kubernetes.io/hostname\",\n                    )\n                ]\n            )\n        )\n\n        # Use k8s_client.V1Toleration to define node tolerations\n        k8s_tolerations = [k8s.V1Toleration(key=\"dedicated\", operator=\"Equal\", value=\"airflow\")]\n\n        # Use k8s_client.V1ResourceRequirements to define resource limits\n        k8s_resource_requirements = k8s.V1ResourceRequirements(\n            requests={\"memory\": \"512Mi\"}, limits={\"memory\": \"512Mi\"}\n        )\n\n        kube_exec_config_resource_limits = {\n            \"pod_override\": k8s.V1Pod(\n                spec=k8s.V1PodSpec(\n                    containers=[\n                        k8s.V1Container(\n                            name=\"base\",\n                            resources=k8s_resource_requirements,\n                        )\n                    ],\n                    affinity=k8s_affinity,\n                    tolerations=k8s_tolerations,\n                )\n            )\n        }\n\n        @task(executor_config=kube_exec_config_resource_limits)\n        def task_with_resource_limits():\n            print_stuff()\n\n        four_task = task_with_resource_limits()\n\n        (\n            start_task()\n            >> [volume_task, other_ns_task, sidecar_task]\n            >> third_task\n            >> [base_image_task, four_task]\n        )\n"
  },
  {
    "fileloc_hash": 34952771498618910,
    "fileloc": "/opt/airflow/dags/fharenheit.py",
    "last_updated": "2024-05-19 07:18:47.320000 +00:00",
    "source_code": "from typing import List, Dict, Any, Optional\nfrom datetime import datetime, timedelta\nfrom textwrap import dedent\nfrom airflow.models import TaskInstance\nfrom airflow.utils.state import State\nfrom airflow.settings import Session\nfrom airflow.models.taskinstance import TaskInstance\nfrom pprint import pprint\n\n#==============this code added==================================================================:\n#import pydevd_pycharm\n\n#pydevd_pycharm.settrace('10.0.1.202', port=12345, stdoutToServer=True, stderrToServer=True)\n#================================================================================================\n\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow import DAG\n\n# Operators; we need this to operate!\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import (\n    ExternalPythonOperator,\n    PythonOperator,\n    PythonVirtualenvOperator,\n    is_venv_installed,\n)\n\nwith DAG(\n    'fharenheit',\n    # These args will get passed on to each operator\n    # You can override them on a per-task basis during operator initialization\n    default_args={\n        'depends_on_past': False,\n        'email': ['airflow@example.com'],\n        'email_on_failure': False,\n        'email_on_retry': False,\n        'retries': 0,\n        'retry_delay': timedelta(minutes=5),\n        # 'queue': 'bash_queue',\n        # 'pool': 'backfill',\n        # 'priority_weight': 10,\n        # 'end_date': datetime(2016, 1, 1),\n        # 'wait_for_downstream': False,\n        # 'sla': timedelta(hours=2),\n        # 'execution_timeout': timedelta(seconds=300),\n        # 'on_failure_callback': some_function,\n        # 'on_success_callback': some_other_function,\n        # 'on_retry_callback': another_function,\n        # 'sla_miss_callback': yet_another_function,\n        # 'trigger_rule': 'all_success'\n    },\n    description='A simple tutorial DAG',\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['']\n) as dag:\n\n    def get_previous_success_timestamp(ds=None, **context):\n        print(\"Context  \")\n        print(context['prev_data_interval_end_success'])\n        print((context['prev_data_interval_end_success']).format('YYYY-MM-DD HH:mm:ss.SSS'))\n        return context['prev_data_interval_end_success']\n\n    def print_context(ds=None, **context):\n        \"\"\"Print the Airflow context and ds variable from the context.\"\"\"\n        print(\"::group::All kwargs\")\n        pprint(context)\n        print(\"::endgroup::\")\n        print(\"::group::Context variable ds\")\n        print(ds)\n        print(\"::endgroup::\")\n        return \"Whatever you return gets printed in the logs\"\n\n    def last_execution_date(dag_id: str, task_id: str, n: int):\n        session = Session()\n        query_val = (\n            session.query(TaskInstance)\n            .filter(\n                TaskInstance.dag_id == dag_id,\n                TaskInstance.task_id == task_id,\n                TaskInstance.state == State.SUCCESS,\n            )\n            .order_by(TaskInstance.execution_date.desc())\n            .limit(n)\n        )\n        execution_dates = list(map(lambda ti: ti.execution_date, query_val))\n        return execution_dates\n\n\n    # print(task_instance)\n\n    print(\" .\")\n\n    t2 = PythonOperator(task_id=\"get_previous_success_timestamp\", python_callable=get_previous_success_timestamp)\n\n    print(t2)\n\n\n    # t1, t2 and t3 are examples of tasks created by instantiating operators\n    t1 = BashOperator(\n        task_id='print_date',\n        bash_command='date',\n    )\n\n    t2 = PythonOperator(task_id=\"print_the_context\", python_callable=print_context)\n\n\n    # print(last_execution_date('fharenheit', 'print_date', 1))\n\n    t1 >> t2\n"
  }
]